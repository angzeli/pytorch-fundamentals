{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f518e2-5430-46bb-94f7-0bbb132a8c4c",
   "metadata": {},
   "source": [
    "# Scope of this notebook\n",
    "\n",
    "This notebook introduces a **minimal learning problem** in PyTorch, focusing on how gradient-based learning works at a mechanical level.  \n",
    "It demonstrates how model parameters are updated through repeated application of forward computation, loss evaluation, backpropagation, and parameter updates.\n",
    "\n",
    "The emphasis is on developing **clear intuition** for:\n",
    "- how losses are defined and computed,\n",
    "- how gradients are obtained using PyTorch’s automatic differentiation engine,\n",
    "- how parameter updates lead to learning over multiple iterations,\n",
    "- and how the learning rate affects convergence behaviour.\n",
    "\n",
    "All concepts are illustrated using explicit tensor operations and a fully manual training loop, without relying on higher-level abstractions.\n",
    "\n",
    "This notebook is intended as a **conceptual bridge** between basic tensor operations and more advanced PyTorch workflows.  \n",
    "Readers should finish this notebook with a solid understanding of *what learning means computationally*, before introducing optimisers or neural network modules.\n",
    "\n",
    "It does **not** introduce `nn.Module`, built-in optimisers, datasets, or data loaders.  \n",
    "These abstractions will be introduced in subsequent notebooks once the underlying mechanics are well understood.\n",
    "\n",
    "**Recommended prerequisites:** Familiarity with PyTorch tensor fundamentals, basic Python programming, and elementary linear algebra.\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Angze Li\n",
    "\n",
    "**Last updated:** 2026-02-01\n",
    "\n",
    "**Version:** v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d26685db-2781-4046-88a8-b2c42d5ef99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad008c21-7229-439e-bd87-f464c7a4dd80",
   "metadata": {},
   "source": [
    "### What problem are we setting up?\n",
    "\n",
    "We consider a very small model with:\n",
    "- an input vector `x`\n",
    "- trainable parameters `w` and `b`\n",
    "- a loss function that measures how wrong the model output is\n",
    "\n",
    "The model produces an output by combining the input with its parameters:\n",
    "\n",
    "- `w` (the **weights**) determine how strongly each component of the input `x` influences the output.\n",
    "- `b` (the **bias**) provides an offset that allows the model to shift its output independently of the input.\n",
    "- Together, `w` and `b` define the behaviour of the model and are the quantities we want to learn.\n",
    "\n",
    "The **loss function** compares the model output to the expected output and returns a single scalar value that quantifies the error.  \n",
    "This scalar loss is what drives learning: by analysing how the loss changes with respect to `w` and `b`, we can adjust the parameters to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e9737db3-2299-4668-8fe7-d1eae3163ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and target\n",
    "x = torch.ones(5)      # input features\n",
    "y = torch.zeros(3)    # expected output (target)\n",
    "\n",
    "# Trainable parameters\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584dbdf-cab7-4e65-9191-f63f87e22a18",
   "metadata": {},
   "source": [
    "### What these tensors represent\n",
    "- `x` is a fixed input vector.\n",
    "- `w` is a weight matrix mapping 5 inputs to 3 outputs.\n",
    "- `b` is a bias vector added to the output. (Note the dimensionalities of `x`, `w` and `b`.)\n",
    "- `requires_grad=True` tells PyTorch:\n",
    "  \n",
    "  > “Track how this tensor influences the final loss.”\n",
    "\n",
    "Only tensors with `requires_grad=True` will receive gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8ca49d8-d7cc-4b6c-aeaa-66e508e02fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output z: tensor([ 0.8504, -1.8232,  3.8355], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(\"Model output z:\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f6133-2872-4044-b2bf-e2ac9ee8ac69",
   "metadata": {},
   "source": [
    "### What just happened mathematically\n",
    "\n",
    "This line computes the **forward pass** of the model:\n",
    "\n",
    "$$\n",
    "z = x W + b\n",
    "$$\n",
    "\n",
    "- `x` is shape `(5,)`\n",
    "- `w` is shape `(5, 3)`\n",
    "- result `z` is shape `(3,)`\n",
    "\n",
    "Each element of `z` depends on **all entries of `w` and `b`**.\n",
    "\n",
    "PyTorch remembers this dependency internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8a2a895f-737e-49a5-9f08-169b7dc6e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(1.7376, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349a5ac-eb4e-48ff-b388-02c4db1d6b1c",
   "metadata": {},
   "source": [
    "### Why we need a loss\n",
    "\n",
    "The loss is a **single scalar number** that measures how far the model output `z`\n",
    "is from the target `y`.\n",
    "\n",
    "In this example, we use **binary cross-entropy with logits** (hence the mame of the function `torch.nn.functional.binary_cross_entropy_with_logits()`) , which is computed *element-wise* as:\n",
    "\n",
    "$$\n",
    "\\ell(z_i, y_i) = - \\left[ y_i \\ln(\\sigma(z_i)) + (1 - y_i)\\ln(1 - \\sigma(z_i)) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function,\n",
    "- $z_i$ is the model output (logit) for the $i$-th entry,\n",
    "- $y_i \\in \\{0, 1\\}$ is the corresponding target.\n",
    "\n",
    "The total loss is obtained by averaging over all entries:\n",
    "\n",
    "$$\n",
    "\\text{loss} = \\frac{1}{N} \\sum_i \\ell(z_i, y_i)\n",
    "$$\n",
    "\n",
    "Key points:\n",
    "- The loss is a **scalar**, which is required for backpropagation.\n",
    "- Using logits (raw values `z`) instead of probabilities is **numerically more stable**.\n",
    "- PyTorch combines the sigmoid operation and cross-entropy computation internally.\n",
    "\n",
    "At this point, PyTorch has built a **computation graph** linking  \n",
    "`w, b → z → loss`.\n",
    "\n",
    "One can manually verify that the loss computed above follows this formula exactly by applying the sigmoid function to $z$ and evaluating the expression entry by entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2a8384c8-66aa-494e-a527-d94ca0144bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w before backward: None\n",
      "Gradient of b before backward: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient of w before backward:\", w.grad)\n",
    "print(\"Gradient of b before backward:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4be57-8e53-4ab7-8619-31e86c03a3c2",
   "metadata": {},
   "source": [
    "### Why gradients are `None`\n",
    "\n",
    "Gradients are not computed automatically.\n",
    "\n",
    "Up to now, we have only done a **forward pass**.\n",
    "No differentiation has happened yet.\n",
    "\n",
    "To compute gradients, we must explicitly ask PyTorch to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "600dca61-faef-4cad-ac6c-a7a7292b2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c362ead-8eec-4296-8a80-24763e5b1c57",
   "metadata": {},
   "source": [
    "### What `backward()` actually means\n",
    "\n",
    "Calling `loss.backward()` tells PyTorch to compute **how the loss changes with respect to each trainable parameter**.\n",
    "\n",
    "More concretely, PyTorch performs the following steps:\n",
    "\n",
    "1. **Start from the scalar `loss`**  \n",
    "   The loss is the final output of the computation graph. Gradients are always computed with respect to a scalar, because only then is the gradient well-defined.\n",
    "\n",
    "2. **Traverse the computation graph backwards**  \n",
    "   PyTorch follows the recorded operations in reverse order, moving from the loss back toward the parameters (`w` and `b`).\n",
    "\n",
    "3. **Apply the chain rule automatically**  \n",
    "   At each operation, PyTorch computes local derivatives and combines them using the chain rule, propagating gradients backward through the graph.\n",
    "\n",
    "4. **Accumulate gradients in leaf tensors**  \n",
    "   For every tensor marked with `requires_grad=True` that is a *leaf* of the graph (such as `w` and `b`), PyTorch stores the resulting gradients in the `.grad` attribute.\n",
    "\n",
    "After this process:\n",
    "- `w.grad` contains $\\partial\\ \\text{loss}/\\partial w$\n",
    "- `b.grad` contains $\\partial\\ \\text{loss}/\\partial b$\n",
    "\n",
    "This entire backward pass from the loss back to the parameters is known as **backpropagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10a4c464-17e8-472c-b168-e7c12fba6280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w:\n",
      " tensor([[0.2336, 0.0464, 0.3263],\n",
      "        [0.2336, 0.0464, 0.3263],\n",
      "        [0.2336, 0.0464, 0.3263],\n",
      "        [0.2336, 0.0464, 0.3263],\n",
      "        [0.2336, 0.0464, 0.3263]])\n",
      "Gradient of b:\n",
      " tensor([0.2336, 0.0464, 0.3263])\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient of w:\\n\", w.grad)\n",
    "print(\"Gradient of b:\\n\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b8c1f-cac5-4ece-a59f-5ce0a564bf9d",
   "metadata": {},
   "source": [
    "### How to interpret these gradients\n",
    "- `w.grad` tells us how the loss changes if each entry of `w` is *increased* slightly.\n",
    "- `b.grad` tells us how the loss changes if each entry of `b` is *increased* slightly.\n",
    "\n",
    "The gradients have the **same shape** as the parameters they belong to.\n",
    "This is not an accident — it allows **parameter-wise** updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc990303-fdc1-4b49-ac32-d2d52b50f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "    b -= learning_rate * b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177bf267-d5f3-46d5-80a6-5b926861c4b4",
   "metadata": {},
   "source": [
    "### Learning rate and `torch.no_grad(`\n",
    "\n",
    "The variable `learning_rate` controls **how large each parameter update is**.  \n",
    "It determines how far we move the parameters in the direction that reduces the loss.\n",
    "\n",
    "- A larger learning rate leads to **bigger steps**, which may speed up learning but risk overshooting.\n",
    "- A smaller learning rate leads to **smaller, more cautious steps**, which are more stable but slower.\n",
    "\n",
    "In this example, `learning_rate = 0.1` is chosen as a simple, reasonable value to clearly illustrate the update step.\n",
    "\n",
    "---\n",
    "\n",
    "We use `torch.no_grad()` because:\n",
    "- Updating parameters is **not** part of the model itself\n",
    "- We do not want PyTorch to track these operations\n",
    "\n",
    "Otherwise, the computation graph would grow incorrectly.\n",
    "\n",
    "Expand the two cells below to learn more about learning rate and the necessity of `torch.no_grad()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2b982-acc3-4dde-bb76-77abc4a0e20c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Learning rate: scale, intuition, and practical behaviour</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### What is the usual range of the learning rate?\n",
    "\n",
    "There is no single “correct” learning rate, but in practice:\n",
    "\n",
    "- Common values range from **1e−4 to 1e−1**\n",
    "- Typical starting points are **0.1**, **0.01**, or **0.001**\n",
    "- The appropriate value depends on:\n",
    "  - the model,\n",
    "  - the loss function,\n",
    "  - and the scale of the gradients\n",
    "\n",
    "In this notebook, we use `learning_rate = 0.1` purely for illustration, not because it is optimal.\n",
    "\n",
    "---\n",
    "\n",
    "#### Intuition: the hill analogy\n",
    "\n",
    "Imagine the loss as a landscape with hills and valleys:\n",
    "\n",
    "- The **height** represents the loss value\n",
    "- The **slope** represents the gradient\n",
    "- The parameters (`w`, `b`) represent your current position on the hill\n",
    "\n",
    "The learning rate controls **how large a step you take downhill**:\n",
    "\n",
    "- Too small → you move very slowly and make little progress\n",
    "- Too large → you may overshoot the valley and oscillate\n",
    "- Just right → you move steadily toward a minimum\n",
    "\n",
    "---\n",
    "\n",
    "#### Why the learning rate matters\n",
    "\n",
    "The gradient tells you *which direction* reduces the loss.  \n",
    "The learning rate determines *how far* you move in that direction.\n",
    "\n",
    "Effective learning requires **both** a meaningful gradient and an appropriate learning rate.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014de5ed-f1f0-4a41-bbac-bcef3128db5d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Why <code>torch.no_grad()</code> is needed during parameter updates</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "We use `torch.no_grad()` because updating model parameters is **not part of the forward computation** that defines the model itself.\n",
    "\n",
    "During the forward pass, PyTorch records operations to build a computation graph.  \n",
    "However, parameter updates are a **bookkeeping step** performed after gradients have been computed.\n",
    "\n",
    "If parameter updates were performed without `torch.no_grad()`:\n",
    "- PyTorch would attempt to track these updates as part of the computation graph\n",
    "- The graph would incorrectly include the update operations themselves\n",
    "- Subsequent calls to `backward()` could fail or produce incorrect gradients\n",
    "\n",
    "Using `torch.no_grad()` temporarily disables gradient tracking, ensuring that:\n",
    "- parameter updates do not become part of the computation graph\n",
    "- each training step starts from a clean graph\n",
    "- gradients are computed only for the forward computation\n",
    "\n",
    "This separation between **model computation** and **parameter updates** is essential for correct and stable training.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8eaab674-fbec-4f82-b462-4bbd7b56127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " tensor([[ 2.1033, -0.1954,  1.2999],\n",
      "        [-0.0387, -0.0643,  1.8156],\n",
      "        [ 0.9359, -1.0646, -1.2804],\n",
      "        [-0.1207,  1.5824, -1.0933],\n",
      "        [-1.6452,  0.0147,  0.4893]], requires_grad=True)\n",
      "b:\n",
      " tensor([-0.5243, -2.1238,  2.4086], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "print(\"w:\\n\", w)\n",
    "print(\"b:\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3636e4-b723-49ba-b472-1954cc187208",
   "metadata": {},
   "source": [
    "### Why gradients must be cleared\n",
    "\n",
    "By default, PyTorch **accumulates gradients**.\n",
    "\n",
    "This means:\n",
    "- Calling `backward()` multiple times without resetting gradients will **add new gradients** to the existing values stored in `.grad`\n",
    "- This behaviour is intentional and allows gradients to be accumulated across multiple backward passes (e.g. when summing losses)\n",
    "\n",
    "However, for most training loops—especially when learning—this accumulation can be confusing and lead to subtle bugs.\n",
    "\n",
    "Clearing gradients explicitly ensures that each backward pass computes gradients **only for the current forward computation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30798d17-b99e-4526-9672-7cda106dfdfc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a2516-51fd-4bb8-865d-63e34dc6c9f0",
   "metadata": {},
   "source": [
    "### Make it a loop\n",
    "\n",
    "So far, we have seen how to compute gradients and perform a **single parameter update**.\n",
    "\n",
    "Learning, however, does not happen in one step.  \n",
    "Instead, the process of:\n",
    "1. computing the loss,\n",
    "2. computing gradients via backpropagation,\n",
    "3. updating parameters,\n",
    "\n",
    "is repeated **many times**.\n",
    "\n",
    "Each repetition is called a **training step** (or iteration).  \n",
    "Over successive steps, the parameters are gradually adjusted to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "48e1c3d0-100c-463b-86a3-e0ddb3e59553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0 | Loss: 1.640532\n",
      "Step  1 | Loss: 1.546891\n",
      "Step  2 | Loss: 1.456637\n",
      "Step  3 | Loss: 1.369751\n",
      "Step  4 | Loss: 1.286210\n",
      "Step  5 | Loss: 1.206000\n",
      "Step  6 | Loss: 1.129122\n",
      "Step  7 | Loss: 1.055596\n",
      "Step  8 | Loss: 0.985460\n",
      "Step  9 | Loss: 0.918764\n",
      "Step 10 | Loss: 0.855570\n",
      "Step 11 | Loss: 0.795937\n",
      "Step 12 | Loss: 0.739914\n",
      "Step 13 | Loss: 0.687527\n",
      "Step 14 | Loss: 0.638773\n",
      "Step 15 | Loss: 0.593612\n",
      "Step 16 | Loss: 0.551963\n",
      "Step 17 | Loss: 0.513707\n",
      "Step 18 | Loss: 0.478688\n",
      "Step 19 | Loss: 0.446725\n"
     ]
    }
   ],
   "source": [
    "# Simple training loop to demonstrate learning\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_steps = 20\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Forward pass\n",
    "    z = torch.matmul(x, w) + b\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameter update\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Clear gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # Monitor progress\n",
    "    print(f\"Step {step:2d} | Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60593910-18e6-49ad-99e3-89e1ed30d115",
   "metadata": {},
   "source": [
    "### The training loop: how learning emerges\n",
    "- Each loop iteration performs one **forward–backward–update** cycle.\n",
    "- The loss is recomputed using the updated parameters at every step.\n",
    "- If the learning rate is reasonable, the loss should gradually decrease.\n",
    "\n",
    "This loop demonstrates the core mechanism behind training neural networks:\n",
    "repeated application of backpropagation and parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b435b-1a03-45c0-9828-70e0f5a7d721",
   "metadata": {},
   "source": [
    "### What backpropagation really did here\n",
    "\n",
    "- We computed a forward pass to get a scalar loss\n",
    "- PyTorch recorded how that loss depends on parameters\n",
    "- `loss.backward()` computed all required gradients automatically\n",
    "- Gradients told us how to update parameters to reduce the loss\n",
    "\n",
    "No neural network magic — just calculus applied systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ac39cc-4195-4f74-9eab-98c0c45594a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260defd0-928d-4eff-9bac-c7c028642906",
   "metadata": {},
   "source": [
    "### Convergence in gradient-based learning\n",
    "\n",
    "At each iteration:\n",
    "1. A forward pass computes the model output and the loss.\n",
    "2. Backpropagation computes gradients of the loss with respect to the parameters.\n",
    "3. The parameters are updated in the direction that reduces the loss.\n",
    "4. Gradients are cleared in preparation for the next step.\n",
    "\n",
    "As the loop progresses, the loss stored in `loss_history` decreases and eventually falls below the chosen threshold.  \n",
    "This indicates that successive parameter updates are making **smaller and smaller improvements** (see the figure below for the visualisation of decrease in `loss`), and the model is approaching a minimum of the loss function.\n",
    "\n",
    "The stopping condition is:\n",
    "```python\n",
    "loss_value > 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef2b3054-0ed7-412f-bcc1-ab9dcbf567f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0 | Loss: 0.900455\n",
      "Step  100 | Loss: 0.045342\n",
      "Step  200 | Loss: 0.023171\n",
      "Step  300 | Loss: 0.015711\n",
      "Step  400 | Loss: 0.011919\n",
      "\n",
      "Total learning steps: 481\n",
      "Final loss: 0.009998\n",
      "CPU times: user 31.7 ms, sys: 51.3 ms, total: 82.9 ms\n",
      "Wall time: 41.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3) \n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "loss_value = float(\"inf\")\n",
    "learning_step = 0\n",
    "max_steps = 1000\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "while loss_value > 0.01 and learning_step < max_steps:\n",
    "    # Forward pass\n",
    "    z = torch.matmul(x, w) + b\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameter update\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Clear gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # Extract scalar loss\n",
    "    loss_value = loss.item()\n",
    "    loss_history.append(loss_value)\n",
    "\n",
    "    # Monitor progress\n",
    "    if learning_step % 100 == 0:\n",
    "        print(f\"Step {learning_step:4d} | Loss: {loss_value:.6f}\")\n",
    "\n",
    "    learning_step += 1\n",
    "\n",
    "print(\"\\nTotal learning steps:\", learning_step)\n",
    "print(\"Final loss:\", round(loss_value, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "60be9273-16b0-4fa6-866d-e1bb3bd1fd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss convergence over training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5ee24-6238-43e5-a5f6-e765cf113666",
   "metadata": {},
   "source": [
    "### The effect of learning rate\n",
    "The experiment below examines how the **learning rate** affects convergence in gradient-based learning.\n",
    "\n",
    "For each value in `learning_rates`, the model is re-initialised and trained under identical conditions. This ensures that any differences in behaviour arise solely from the choice of learning rate, rather than from initialisation or model structure.\n",
    "\n",
    "The reported results show how many steps are required for the loss to fall below the threshold\n",
    "```python\n",
    "loss_value > 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "776c10ce-10d8-4aaa-8433-a624d48b82ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate = 0.001 | steps = 1000 | final loss = 1.650006\n",
      "learning_rate = 0.01  | steps = 1000 | final loss = 0.054871\n",
      "learning_rate = 0.1   | steps = 483  | final loss = 0.010000\n",
      "learning_rate = 1.0   | steps = 51   | final loss = 0.009928\n",
      "learning_rate = 10    | steps = 2    | final loss = 0.004450\n",
      "learning_rate = 100   | steps = 2    | final loss = 0.000000\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "max_steps = 1000\n",
    "threshold = 0.01\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Re-initialise model for fair comparison\n",
    "    x = torch.ones(5)\n",
    "    y = torch.zeros(3) \n",
    "    w = torch.randn(5, 3, requires_grad=True)\n",
    "    b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "    loss_value = float(\"inf\")\n",
    "    learning_step = 0\n",
    "    loss_history = []\n",
    "\n",
    "    while loss_value > threshold and learning_step < max_steps:\n",
    "        # Forward pass\n",
    "        z = torch.matmul(x, w) + b\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameter update\n",
    "        with torch.no_grad():\n",
    "            w -= lr * w.grad\n",
    "            b -= lr * b.grad\n",
    "\n",
    "        # Clear gradients\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "        # Extract scalar loss\n",
    "        loss_value = loss.item()\n",
    "        loss_history.append(loss_value)\n",
    "\n",
    "        learning_step += 1\n",
    "\n",
    "    print(f\"learning_rate = {lr:<5} | steps = {learning_step:<4} | final loss = {loss_value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862089a-094c-4a2a-8e06-3c42d28a4e1b",
   "metadata": {},
   "source": [
    "### Interpreting learning rate effects\n",
    "\n",
    "In this simple and well-conditioned example, larger learning rates reduce the number of steps required for convergence, allowing the loss to fall below the chosen threshold more quickly.\n",
    "\n",
    "However, this behaviour does not generalise to all optimisation problems.  In more complex loss landscapes, overly large learning rates can lead to:\n",
    "- unstable updates,\n",
    "- divergence of the loss,\n",
    "- or convergence to poor solutions that generalise poorly.\n",
    "\n",
    "For this reason, smaller learning rates are often preferred in practice, as they provide greater **robustness and stability** across a wide range of models and training conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ea2f0-60ef-41c3-8124-7e296a740776",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section, we examined a minimal learning problem and followed the entire training process step by step.\n",
    "\n",
    "Starting from a simple linear model, we:\n",
    "- defined a loss function,\n",
    "- used backpropagation to compute gradients,\n",
    "- updated parameters using gradient descent,\n",
    "- and observed convergence over multiple iterations.\n",
    "\n",
    "This example captures the essential mechanics underlying gradient-based learning.\n",
    "In the next sections, these same ideas will be reused and extended using higher-level PyTorch abstractions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
