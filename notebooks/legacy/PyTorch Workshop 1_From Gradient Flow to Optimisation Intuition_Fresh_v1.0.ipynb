{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec51929b-f6c6-49a0-b2d0-833433114aeb",
   "metadata": {},
   "source": [
    "# ðŸ§ª Workshop 1: From Gradient Flow to Optimisation Intuition\n",
    "\n",
    "This workshop builds on Tutorials 1â€“4 and closes **Part 1** of the series, consolidating core ideas about PyTorch `autograd` and gradient flow before moving on to explicit optimisation in Workshop 2 and Part 2.\n",
    "\n",
    "Rather than treating gradients as black-box training signals, the workshop emphasises **gradients as sensitivity measures**, especially in settings where model outputs are **tensor-valued** and gradients are computed using **explicit upstream directions**.\n",
    "\n",
    "The problems are designed to shift perspective:\n",
    "- from *â€œwhat gradient does PyTorch give me?â€*\n",
    "- to *â€œwhat does this gradient represent, and why?â€*\n",
    "\n",
    "The emphasis is on developing intuition for:\n",
    "- how gradients flow through structured linear and nonlinear computations,\n",
    "- how upstream gradients select directions in output space,\n",
    "- how `.grad` reflects sensitivity rather than optimisation,\n",
    "- and how gradient structure anticipates optimisation behaviour.\n",
    "\n",
    "Key ideas explored include:\n",
    "- vectorâ€“Jacobian products as the fundamental object computed by `backward(v)`,\n",
    "- the role of upstream gradients in shaping gradient flow,\n",
    "- sparsity and structure in gradients induced by linear maps and nonlinearities (e.g., ReLU),\n",
    "- interpreting gradients statistically and geometrically rather than procedurally,\n",
    "- and using controlled experiments to reason about gradient behaviour.\n",
    "\n",
    "This workshop serves as a conceptual bridge between:\n",
    "- `autograd` mechanics and gradient flow (Tutorials 3â€“4),\n",
    "- and **objective design and optimisation dynamics** (Workshop 2 and Part 2).\n",
    "\n",
    "The focus is intentionally *not* on training pipelines, datasets, or optimisers, but on understanding how gradients behave in controlled settingsâ€”laying the groundwork for effective optimisation.\n",
    "\n",
    "Recommended prerequisites:\n",
    "- Familiarity with PyTorch tensors and `.backward()`\n",
    "- Understanding of scalar vs tensor-valued gradients\n",
    "- Basic comfort with linear algebra and nonlinear activations\n",
    "\n",
    "---\n",
    "\n",
    "**Author: Angze Li**\n",
    "\n",
    "**Last updated: 2026-02-06**\n",
    "\n",
    "**Version: v1.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a2f5c-2c82-4a64-9514-14869acffc75",
   "metadata": {},
   "source": [
    "## ðŸ§© Problem 1: Sensitivity of a Feature Transformation (Warm-up)\n",
    "\n",
    ">In many models, inputs are transformed into feature representations before being used by a downstream objective.\n",
    "> Understanding which inputs influence which features is critical for interpretability and optimisation.\n",
    "\n",
    "Consider the following setup:\n",
    "```python\n",
    "x = torch.randn(6, requires_grad=True)\n",
    "\n",
    "W = torch.tensor([\n",
    "    [1.0,  0.0,  0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0,  1.0,  0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0,  0.0,  2.0, 0.0, 0.0, 0.0],\n",
    "    [0.0,  0.0,  0.0, 3.0, 0.0, 0.0],\n",
    "], requires_grad=False)\n",
    "\n",
    "out = torch.relu(W @ x) # what does this line do?\n",
    "```\n",
    "This produces a 4-dimensional tensor output, not a scalar.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Task\n",
    "1.\tConstruct an upstream gradient vector `v` such that:\n",
    "    - the last feature of out is weighted most heavily,\n",
    "    - the first two features are ignored.\n",
    "2. Call:\n",
    "```python\n",
    "out.backward(v)\n",
    "```\n",
    "3. Inspect `x.grad`.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Questions to think about\n",
    "- Which components of `x` receive non-zero gradients?\n",
    "- How does the structure of `W` affect gradient flow?\n",
    "- How does ReLU change which inputs are â€œactiveâ€?\n",
    "- Why is `x.grad` sparse in some cases?\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Hint (optional)\n",
    "\n",
    "> Think in terms of which inputs influence which outputs, and which outputs are being emphasised by `v`.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Learning outcomes\n",
    "\n",
    "After this problem, you should be comfortable with:\n",
    "- interpreting `.grad` as a sensitivity map,\n",
    "- reasoning about gradient flow through linear + nonlinear layers,\n",
    "- understanding how upstream weighting reshapes gradient influence.\n",
    "\n",
    "This problem bridges Tutorials 3 and 4 cleanly and warms up the optimisation mindset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be29b7c-684b-4d89-89ea-e7f082b621fd",
   "metadata": {},
   "source": [
    "## Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc001b-8ec8-4b2c-b068-2b6474f1437b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
