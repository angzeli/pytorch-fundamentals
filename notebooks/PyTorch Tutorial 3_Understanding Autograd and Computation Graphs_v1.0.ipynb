{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7eb92f-c6cc-4719-b2ec-70ab6e504544",
   "metadata": {},
   "source": [
    "# Scope of this notebook\n",
    "\n",
    "This notebook builds on earlier tutorials and focuses on understanding how PyTorch autograd represents and traverses computation graphs.\n",
    "\n",
    "In particular, it introduces the **directed acyclic graph (DAG)** underlying automatic differentiation and shows how gradients are propagated through this graph during backpropagation.\n",
    "\n",
    "The emphasis is on developing conceptual intuition for:\n",
    "- how computation graphs are constructed dynamically,\n",
    "- how backward functions (`grad_fn`) are linked together,\n",
    "- how gradients flow through operations and terminate at leaf tensors,\n",
    "- and how PyTorch supports higher-order differentiation.\n",
    "\n",
    "Key topics covered include:\n",
    "- the structure of computation graphs (nodes, edges, roots, and leaves),\n",
    "- the role of `grad_fn` and `next_functions`,\n",
    "- tracing gradient flow through the DAG,\n",
    "- dynamic graph construction in PyTorch (define-by-run),\n",
    "- and computing higher-order derivatives using `create_graph=True`.\n",
    "\n",
    "This notebook is intended as a conceptual bridge between:\n",
    "- basic gradient-based learning (Tutorial 2),\n",
    "- and more advanced topics such as Jacobians, Hessians, and optimisation methods.\n",
    "\n",
    "It does not cover neural network modules, datasets, optimisers, or training pipelines, which are better addressed separately once the foundations of autograd are firmly understood.\n",
    "\n",
    "Recommended prerequisites:\n",
    "- Familiarity with PyTorch tensors\n",
    "- Basic understanding of backpropagation and first-order gradients\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Angze Li\n",
    "\n",
    "**Last updated:** 2026-02-03\n",
    "\n",
    "**Version:** v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6088f04-c6ca-4405-a2a5-7f3f77241998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf2b81-31b7-4001-8a40-d14b08364b5b",
   "metadata": {},
   "source": [
    "### Understanding the Computational Graph (DAG) in `Autograd`\n",
    "In the context of deep learning, `Autograd` uses a **Directed Acyclic Graph (DAG)** to keep track of every operation performed on your data. This graph is the blueprint that allows the system to calculate gradients through *backpropagation*.\n",
    "\n",
    "Key Components of the DAG:\n",
    "\n",
    "- **Nodes**: These represent **Tensors** (the data) and **Functions** (the operations like addition, multiplication, or activation functions).\n",
    "\n",
    "- **Edges**: These represent the flow of data from one operation to the next.\n",
    "\n",
    "- **Leaves**: The input tensors (usually your **model parameters/weights**).\n",
    "\n",
    "- **Roots**: The final output tensor (usually your **Loss value**).\n",
    "\n",
    "Why **\"Directed\"** and **\"Acyclic\"**?\n",
    "\n",
    "- **Directed**: The data flows in a specific direction—from inputs to outputs during the forward pass, and from the loss back to the inputs during the backward pass.\n",
    "\n",
    "- **Acyclic**: The graph cannot have loops. If it did, the chain rule would enter an infinite loop, and you could never finish calculating a gradient.\n",
    "\n",
    "The Two Phases:\n",
    "\n",
    "1. **Forward Pass**: You perform math. Autograd records these operations and builds the grad_fn (gradient function) for each resulting tensor.\n",
    "\n",
    "2. **Backward Pass**: You call `.backward()` on the root. Autograd traverses the graph in reverse, applying the Chain Rule to compute how much each leaf tensor contributed to the final result."
   ]
  },
  {
   "attachments": {
    "b2576c42-e123-45a0-8f7d-b6f8712f44c8.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a1233325-bc6f-40be-ac13-84863d73bb8c",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"attachment:b2576c42-e123-45a0-8f7d-b6f8712f44c8.png\"\n",
    "       width=\"300\"\n",
    "       style=\"display: block; margin: auto;\">\n",
    "  <p style=\"text-align: center; font-size: 0.9em;\">\n",
    "    <strong>Figure 1.</strong> Example of a directed acyclic graph. Figure taken from Wikipedia.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db0e127-9c45-40d5-a6b8-85b249507675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass Result (Loss): 2.6581220626831055\n",
      "Leaf grad before backward: x.grad = None\n",
      "------------------------------\n",
      "Gradient of Loss w.r.t x:\n",
      "tensor([[ 0.0056,  0.4392, -0.4450],\n",
      "        [ 0.0056,  0.4392, -0.4450]])\n",
      "Gradient of Loss w.r.t w:\n",
      "tensor([[ 0.0911,  0.0911],\n",
      "        [-0.0316, -0.0316],\n",
      "        [-0.3998, -0.3998]])\n",
      "------------------------------\n",
      "Root (Loss) Op: <MeanBackward0 object at 0x1089df130>\n",
      "Next Op (Add):  <AddBackward0 object at 0x1089dee90>\n",
      "Next Op (MM):   <MmBackward0 object at 0x1089df130>\n"
     ]
    }
   ],
   "source": [
    "# 1. Inputs: 2x3 Matrix (e.g., a batch of 2 inputs with 3 features each)\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "\n",
    "# 2. Weights: 3x2 Matrix (to transform output to size 2x2)\n",
    "w = torch.randn(3, 2, requires_grad=True)\n",
    "\n",
    "# 3. Forward Pass: Matrix Multiplication (mm)\n",
    "y = torch.mm(x, w) \n",
    "\n",
    "# 4. Addition (Bias)\n",
    "z = y + 2.0\n",
    "\n",
    "# 5. Reduction: Most DAGs end in a scalar 'Loss'\n",
    "# The mean() operation is a node in the DAG!\n",
    "loss = z.mean()\n",
    "\n",
    "print(f\"Forward Pass Result (Loss): {loss.item()}\")\n",
    "print(f\"Leaf grad before backward: x.grad = {x.grad}\") # Returns None\n",
    "\n",
    "# 4. The Backward Pass\n",
    "# This populates the .grad attributes throughout the graph\n",
    "loss.backward()\n",
    "\n",
    "# 5. Inspecting the Gradients\n",
    "print(\"-\" * 30)\n",
    "print(f\"Gradient of Loss w.r.t x:\\n{x.grad}\")\n",
    "print(f\"Gradient of Loss w.r.t w:\\n{w.grad}\")\n",
    "\n",
    "# 6. Checking the 'grad_fn' chain (The DAG structure)\n",
    "print(\"-\" * 30)\n",
    "print(f\"Root (Loss) Op: {loss.grad_fn}\")\n",
    "print(f\"Next Op (Add):  {loss.grad_fn.next_functions[0][0]}\")\n",
    "print(f\"Next Op (MM):   {loss.grad_fn.next_functions[0][0].next_functions[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc64416-df06-45db-9a24-b711de0a3086",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Understanding <code>loss.grad_fn.next_functions</code></strong></summary>\n",
    "<br>\n",
    "\n",
    "After computing the loss and calling `backward()`, we inspected part of the computation graph using:\n",
    "```python\n",
    "loss.grad_fn.next_functions[0][0]\n",
    "```\n",
    "To understand what this means, it helps to recall how PyTorch represents the computation graph internally.\n",
    "\n",
    "**The computation graph as linked functions**\n",
    "\n",
    "Each tensor created by an operation stores a reference to a **gradient function** (`grad_fn`) that knows how to propagate gradients backward through that operation.\n",
    "\n",
    "These gradient functions are connected together in a directed acyclic graph (DAG):\n",
    "- The **root** of the graph is the scalar loss\n",
    "- The **leaves** are tensors created by the user with `requires_grad=True`\n",
    "- Each `grad_fn` points to the gradient functions of its **inputs**\n",
    "\n",
    "What is `next_functions`?\n",
    "The attribute `next_functions` is a list of tuples that describe where gradients should flow next during backpropagation.\n",
    "\n",
    "Each entry in `next_functions` corresponds to one input of the operation that produced the current tensor. Concretely:\n",
    "```python\n",
    "loss.grad_fn.next_functions\n",
    "```\n",
    "returns a list of (Function, index) pairs:\n",
    "- `Function` is the backward operation associated with an input\n",
    "- `index` indicates which output of that function is being referenced\n",
    "\n",
    "**Breaking down `loss.grad_fn.next_functions[0][0]`**\n",
    "Let’s interpret this expression step by step:\n",
    "- `loss.grad_fn` → the backward function for the operation that produced loss (in this case, MeanBackward)\n",
    "- `.next_functions` → a list of gradient functions corresponding to the inputs of mean()\n",
    "- `[0]` select the first input to `mean()` (which is the tensor `z`)\n",
    "- `[0]` again → extract the actual gradient function object (ignoring the index)\n",
    "\n",
    "So:\n",
    "```python\n",
    "loss.grad_fn.next_functions[0][0]\n",
    "```\n",
    "returns the backward function of the operation that produced `z`.\n",
    "\n",
    "**Why is this useful?**\n",
    "\n",
    "By following `next_functions`, you can:\n",
    "- trace how gradients flow backward through the graph,\n",
    "- inspect the sequence of operations applied in the forward pass,\n",
    "- and understand how PyTorch applies the chain rule internally.\n",
    "\n",
    "For example, you can trace backwards through the computation graph by repeatedly following the entries in `next_functions`.  \n",
    "Each `.next_functions[i][0]` points to the backward `Function` associated with the *i-th input* of the current operation.\n",
    "\n",
    "In our case, `loss` was produced by `mean(z)`, so the first (and only) input leads back to the operation that created `z`.  \n",
    "Following that chain again leads to the matrix multiplication node:\n",
    "\n",
    "```python\n",
    "loss.grad_fn.next_functions[0][0].next_functions[0][0]\n",
    "```\n",
    "\n",
    "This is exactly what we did to reach the `MmBackward0` node (the backward function corresponding to `torch.mm(x, w)`).\n",
    "\n",
    "And \n",
    "```python\n",
    "loss.grad_fn.next_functions[0][0].next_functions[1][0]\n",
    "```\n",
    "is calling the backward function for the second input of the addition ($z = y + 2.0$), i.e. the scalar constant 2.0. Because this value is not a tensor and does not require gradients, autograd does not create a node for it, so the corresponding entry is \n",
    "```python\n",
    "None.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "029ea6a5-1771-4ca5-ac60-15038df56c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MeanBackward0 object at 0x305206710>\n",
      "  input[0] →     <AddBackward0 object at 0x306991f90>\n",
      "      input[0] →         <MmBackward0 object at 0x306992380>\n",
      "          input[0] →             <AccumulateGrad object at 0x3052bd180>\n",
      "          input[1] →             <AccumulateGrad object at 0x3052bda50>\n",
      "      input[1] →         None (no grad path)\n"
     ]
    }
   ],
   "source": [
    "def walk_grad_fn(fn, max_depth=50, indent=0, seen=None):\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    if fn is None:\n",
    "        print(\"  \" * indent + \"None (no grad path)\")\n",
    "        return\n",
    "    if id(fn) in seen:\n",
    "        print(\"  \" * indent + f\"{fn} (already seen)\")\n",
    "        return\n",
    "    seen.add(id(fn))\n",
    "\n",
    "    print(\"  \" * indent + str(fn))\n",
    "\n",
    "    if indent >= max_depth:\n",
    "        print(\"  \" * (indent + 1) + \"... (max depth reached)\")\n",
    "        return\n",
    "\n",
    "    for i, (next_fn, _) in enumerate(fn.next_functions):\n",
    "        print(\"  \" * (indent + 1) + f\"input[{i}] →\", end=\" \")\n",
    "        walk_grad_fn(next_fn, max_depth=max_depth, indent=indent + 2, seen=seen)\n",
    "\n",
    "walk_grad_fn(loss.grad_fn, max_depth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaedfe2-a2eb-48c7-ba74-1f71ecc9de1a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Inspecting the computation graph programmatically</code></strong></summary>\n",
    "<br>\n",
    "    \n",
    "The function below provides a simple way to walk through **PyTorch’s computation graph** starting from a given `grad_fn`. Rather than manually chaining multiple **.next_functions** calls, this helper function traverses the graph recursively and prints its structure in a readable, indented format.\n",
    "\n",
    "```python\n",
    "walk_grad_fn(loss.grad_fn, max_depth=50) \n",
    "```\n",
    "\n",
    "**What does this function do?**\n",
    "- It starts from the **root of the computation graph** (here, `loss.grad_fn`).\n",
    "- At each step, it prints the current `backward` Function.\n",
    "- It then follows all entries in `.next_functions`, which represent the inputs to the current operation.\n",
    "- Indentation is used to visually represent the **depth and branching structure** of the graph.\n",
    "\n",
    "This makes the DAG structure explicit, rather than implicit.\n",
    "\n",
    "**Why do we need seen?**\n",
    "Autograd graphs may contain shared subgraphs (for example, when a tensor is reused).\n",
    "\n",
    "To avoid infinite loops or duplicated output, the function keeps track of visited nodes using:\n",
    "```python\n",
    "seen = set()\n",
    "seen.add(id(fn))\n",
    "```\n",
    "\n",
    "If a backward function has already been visited, it is skipped.\n",
    "\n",
    "**What does max_depth control?**\n",
    "The `max_depth` argument limits how deep the traversal goes. This prevents excessive output when working with larger or more complex graphs. One may wish to change the value of `max_depth` based on the complexity of the task.\n",
    "```python\n",
    "if indent >= max_depth:\n",
    "    print(\"... (max depth reached)\")\n",
    "```\n",
    "\n",
    "**How should the output be interpreted?**\n",
    "- Each printed line corresponds to a **backward function** in the computation graph.\n",
    "- Lines labeled `input[i]` → indicate which input of the current operation is being followed.\n",
    "- `None (no grad path)` indicates that gradient flow stops at that input (e.g., constants or non-differentiable values).\n",
    "- Leaf tensors themselves do not appear as functions; instead, they receive gradients in their `.grad` attributes after `backward()` is called.\n",
    "\n",
    "**Why is this useful?**\n",
    "This function is intended purely for learning and introspection. It helps make concrete:\n",
    "- how autograd builds a DAG of operations,\n",
    "- how gradients propagate backward through that DAG,\n",
    "- and why some paths terminate while others continue to leaf tensors.\n",
    "\n",
    "In normal PyTorch usage, you would *rarely* (if ever) need to inspect the graph in this way. However, understanding it at this level provides valuable intuition for how automatic differentiation works under the hood.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f292d-fdf7-4125-982f-b175672a6748",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<details>\n",
    "<summary><strong>Why <code>.grad</code> requires a scalar output</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that `.grad` can only be implicitly created for scalar outputs.\n",
    "\n",
    "In PyTorch, calling `backward()` without any arguments is only valid when the tensor you call it on is a scalar (i.e. has exactly one element). This is why, in the forward pass above, we explicitly reduced `z` to a scalar using:\n",
    "\n",
    "```python\n",
    "loss = z.mean()\n",
    "```\n",
    "\n",
    "**Why is this necessary?**\n",
    "\n",
    "Conceptually, backpropagation computes gradients of the form:\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial x}$$\n",
    "\n",
    "This quantity is only well-defined when **loss** is a single number.\n",
    "\n",
    "If `z` were left as a matrix (or vector), its gradient with respect to `x` would be a Jacobian matrix *(this would be discussed in more detail in the next section)*, not a single gradient tensor. In such cases, PyTorch would not know which combination of gradients you want unless you explicitly specify it.\n",
    "\n",
    "This is why:\n",
    "- PyTorch requires a scalar root to automatically start backpropagation.\n",
    "- Reduction operations such as `mean()`, `sum()`, or `max()` are commonly used to produce this scalar.\n",
    "- Most real training pipelines define a scalar loss function for exactly this reason.\n",
    "\n",
    "**What happens internally?**\n",
    "\n",
    "When `loss.backward()` is called:\n",
    "1. PyTorch treats loss as the root of the computation graph.\n",
    "2. It applies the chain rule backward through each operation (`mean` → `add` → `mm`).\n",
    "3. Gradients are accumulated in the .grad attributes of leaf tensors (`x` and `w`).\n",
    "4. Intermediate tensors store gradient rules via their `grad_fn`, but do not store `.grad` values themselves.\n",
    "\n",
    "This design keeps gradient computation:\n",
    "- mathematically well-defined,\n",
    "- memory-efficient,\n",
    "- and consistent with optimisation algorithms that expect scalar objectives.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf9c2f-f870-41dc-a1bc-5fa2884d7a38",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Dynamic computation graphs in PyTorch</strong></summary>\n",
    "<br>\n",
    "    \n",
    "**DAGs are dynamic in PyTorch.**\n",
    "\n",
    "An important feature of PyTorch’s autograd system is that the computation graph is rebuilt from scratch during every forward pass. After each call to `.backward()`, the graph used for that pass is discarded, and a new graph will be constructed the next time you perform a forward computation.\n",
    "\n",
    "This design has several important consequences:\n",
    "- **Control flow is fully supported**.\n",
    "    - You can use Python if statements, loops, and conditionals in your model.\n",
    "    - Only the operations that are actually executed in a given forward pass appear in the graph.\n",
    "- **Graphs reflect runtime behaviour, not static structure**\n",
    "    - The graph records what happened, not what could happen.\n",
    "    - If a branch of code is skipped, it does not exist in the computation graph.\n",
    "- **Shapes and operations may change between iterations**\n",
    "    - You are free to change tensor shapes, network depth, or even which operations are applied from one iteration to the next, as long as the executed operations are differentiable.\n",
    "\n",
    "This approach is often described as **define-by-run**: the computation graph is defined dynamically as the program runs, rather than being specified in advance.\n",
    "\n",
    "In practice, this makes PyTorch particularly well-suited for:\n",
    "- research and experimentation,\n",
    "- models with complex or data-dependent control flow,\n",
    "- and debugging, since the graph follows normal Python execution.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ca2a9-bbfb-409b-8c7f-87c18a2de8a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843cf06a-14d0-4215-a66f-f25c46db268e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Extra: higher-order differentiation (gradients of gradients)</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "By default, calling `loss.backward()` computes **first-order gradients** (e.g. $\\partial \\text{loss}/\\partial x$).\n",
    "\n",
    "Sometimes we also want **gradients of gradients** (second-order information).  \n",
    "PyTorch can do this by treating the gradient computation itself as a differentiable operation.\n",
    "\n",
    "To enable this, we compute gradients with `create_graph=True`, which tells `autograd` to **build a new graph for the gradient** so we can differentiate it again.\n",
    "\n",
    "The example below computes:\n",
    "1. a first derivative $dy/dx$\n",
    "2. then a second derivative $d^2y/dx^2$\n",
    "3. finally a third derivative $d^3y/dx^3$\n",
    "\n",
    "In the example below, we use a smooth non-linear function built from the **sigmoid activation**, which is commonly used in machine learning models.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b44d405-b226-497d-99f6-4cdaf25d7cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: 0.28746968507766724\n",
      "d2y/dx2: -0.05553218349814415\n",
      "d3y/dx3: -0.15883243083953857\n"
     ]
    }
   ],
   "source": [
    "# Higher-order differentiation demo\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# A smooth, non-linear function common in ML\n",
    "y = torch.sigmoid(x)**2\n",
    "\n",
    "# First derivative: dy/dx\n",
    "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(\"dy/dx:\", dy_dx.item())\n",
    "\n",
    "# Second derivative: d2y/dx2\n",
    "d2y_dx2 = torch.autograd.grad(dy_dx, x, create_graph=True)[0]\n",
    "print(\"d2y/dx2:\", d2y_dx2.item())\n",
    "\n",
    "# Third derivative: d3y/dx3\n",
    "d3y_dx3 = torch.autograd.grad(d2y_dx2, x, create_graph=True)[0]\n",
    "print(\"d3y/dx3:\", d3y_dx3.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e19449-5049-45dd-a7d6-b810740efb26",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Computing higher-order derivatives programmatically</strong></summary>\n",
    "\n",
    "<br>\n",
    "The code below defines a helper function nth_derivative that computes the `i-th derivative` of a **scalar** output with respect to a **scalar** input by repeatedly applying automatic differentiation.\n",
    "```python\n",
    "def nth_derivative(y, x, n):\n",
    "    derivative = y\n",
    "    for i in range(n):\n",
    "        derivative = torch.autograd.grad(\n",
    "            derivative, x, create_graph=True\n",
    "        )[0]\n",
    "    return derivative\n",
    "```\n",
    "**How does this work?**\n",
    "- The function starts from the original scalar output `y`.\n",
    "- At each iteration, it computes the gradient of the current quantity with respect to `x`.\n",
    "- Setting `create_graph=True` ensures that each derivative remains differentiable, allowing further derivatives to be computed.\n",
    "- Repeating this process n times yields the n-th derivative.\n",
    "\n",
    "**Important observations**\n",
    "- Higher-order differentiation in PyTorch is achieved by differentiating gradients themselves.\n",
    "- This approach works naturally for scalar-to-scalar functions.\n",
    "- Each higher-order derivative increases computational cost and graph complexity, so in practice only low-order derivatives (e.g. first or second) are commonly used.\n",
    "\n",
    "This example demonstrates that PyTorch’s autograd engine is not limited to first-order gradients, but can support arbitrary-order differentiation when needed.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "288b536f-abde-4a3a-b567-617911ed168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th derivative: 0.28746968507766724\n",
      "2-th derivative: -0.05553218349814415\n",
      "3-th derivative: -0.15883243083953857\n",
      "4-th derivative: 0.17454887926578522\n"
     ]
    }
   ],
   "source": [
    "def nth_derivative(y, x, n):\n",
    "    \"\"\"\n",
    "    Compute the n-th derivative of scalar y with respect to scalar x.\n",
    "    \"\"\"\n",
    "    derivative = y\n",
    "    for i in range(n):\n",
    "        derivative = torch.autograd.grad(\n",
    "            derivative, x, create_graph=True\n",
    "        )[0]\n",
    "    return derivative\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.sigmoid(x)**2\n",
    "\n",
    "for i in range(1, 5):\n",
    "    d = nth_derivative(y, x, i)\n",
    "    print(f\"{i}-th derivative:\", d.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
