{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d217864-9252-41dd-928d-4acb40ced926",
   "metadata": {},
   "source": [
    "# üß™ Workshop 2: From Gradient Structure to Optimisation Dynamics\n",
    "\n",
    "This workshop builds directly on Workshop 1 and marks the transition from gradient interpretation to explicit optimisation dynamics, opening Part 2 of the series.\n",
    "\n",
    "Where Workshop 1 focused on what gradients are and how they are structured, this workshop focuses on what gradients do when they are applied repeatedly. Gradients are no longer treated as static sensitivity maps, but as drivers of parameter evolution.\n",
    "\n",
    "The central shift in perspective is:\n",
    "- from *‚Äúwhat does this gradient look like?‚Äù*\n",
    "- to *‚Äúwhat happens when I follow it?‚Äù*\n",
    "\n",
    "---\n",
    "\n",
    "**Conceptual emphasis**\n",
    "\n",
    "The workshop develops intuition for:\n",
    "- how a single gradient descent step turns sensitivity into motion,\n",
    "- how repeated local updates accumulate into global behaviour,\n",
    "- how objective structure influences optimisation trajectories,\n",
    "- and how gradient geometry affects stability, speed, and convergence.\n",
    "\n",
    "Rather than introducing full training pipelines, the focus remains on controlled, interpretable systems where optimisation dynamics can be reasoned about directly.\n",
    "\n",
    "--- \n",
    "\n",
    "**Key ideas explored include**:\n",
    "- gradient descent as repeated application of vector‚ÄìJacobian products,\n",
    "- implicit objective functions defined by upstream weighting,\n",
    "- the relationship between gradient magnitude, direction, and parameter motion,\n",
    "- conditioning and anisotropy in gradient-driven updates,\n",
    "- how symmetry, nonlinearity, and curvature shape optimisation paths,\n",
    "- and visualising optimisation as movement through parameter space.\n",
    "\n",
    "--- \n",
    "\n",
    "**How this workshop fits in the series**\n",
    "\n",
    "This workshop serves as the conceptual bridge between:\n",
    "- gradient flow and sensitivity analysis (Workshop 1),\n",
    "- and more advanced optimisation topics such as learning rates, curvature, and second-order effects (later in Part 2).\n",
    "\n",
    "By the end of this workshop, gradient descent is no longer a formula, but a geometric process whose behaviour can be anticipated from gradient structure alone.\n",
    "\n",
    "---\n",
    "\n",
    "**What this workshop deliberately does not cover**\n",
    "- Neural network modules (nn.Module)\n",
    "- Optimisers such as Adam, RMSProp, etc.\n",
    "- Datasets, batching, or training loops\n",
    "\n",
    "Those elements are introduced only after optimisation dynamics are conceptually understood.\n",
    "\n",
    "---\n",
    "\n",
    "**Recommended prerequisites**\n",
    "- Completion of Tutorials 1‚Äì4\n",
    "- Workshop 1: From Gradient Flow to Optimisation Intuition\n",
    "- Comfort with gradients, Jacobians, and basic optimisation ideas\n",
    "- Familiarity with linear algebra and nonlinear mappings\n",
    "\n",
    "---\n",
    "\n",
    "**Author: Angze Li**\n",
    "\n",
    "**Last updated: 2026-02-19**\n",
    "\n",
    "**Version: v1.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb363e6-7af8-40b7-a60b-58e6866654ff",
   "metadata": {},
   "source": [
    "## üß© Problem: Designing an Objective via Upstream Gradients\n",
    "\n",
    "> Optimisation is not only about how to minimise a loss\n",
    "> ‚Äî it is also about what objective you choose.\n",
    "\n",
    "In this problem, you will implicitly define an objective by choosing an upstream gradient.\n",
    "\n",
    "Consider:\n",
    "```python\n",
    "X = torch.randn(5, 3, requires_grad=True)\n",
    "\n",
    "Y = torch.tanh(X @ X.T)\n",
    "```\n",
    "Here:\n",
    "- `Y` is a **5√ó5 tensor** measuring pairwise interactions,\n",
    "- the output is *symmetric and non-scalar*.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Task\n",
    "1. Construct an upstream gradient matrix V such that:\n",
    "    - diagonal entries of Y are emphasised,\n",
    "    - off-diagonal entries are penalised.\n",
    "2. Call:\n",
    "```python\n",
    "Y.backward(V)\n",
    "```\n",
    "3. Inspect `X.grad`.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Questions to think about\n",
    "- What implicit scalar objective are you optimising?\n",
    "- How does changing the diagonal/off-diagonal weighting affect `X.grad`?\n",
    "- Which entries of `X` are encouraged to grow or shrink?\n",
    "- Can you interpret this as encouraging **self-similarity** over **cross-similarity**?\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Hint (optional)\n",
    "\n",
    "> You are not optimising `Y` directly.\n",
    "> You are optimising a **weighted trace-like** functional of `Y`.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why this problem matters (Bridge to Part 2)\n",
    "\n",
    "This problem quietly introduces:\n",
    "- custom objective design,\n",
    "- structure-aware optimisation,\n",
    "- gradients as *design tools*, not just training signals.\n",
    "\n",
    "Without using:\n",
    "- optimisers,\n",
    "- learning rates,\n",
    "- training loops,\n",
    "\n",
    "you have already answered:\n",
    "\n",
    ">‚ÄúIf I *were* to optimise this system, what direction would the parameters move?‚Äù\n",
    "\n",
    "That is exactly the mindset needed for Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30cbac-51bc-461a-aab1-f8cb64a580b8",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1b146-a022-4bc7-9ca6-fd8b9f51d6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d65b376-d435-4a8c-9f6a-b46a64be809e",
   "metadata": {},
   "source": [
    "**After you have finished this problem, please refer to the full version of this notebook for the model answer and the trailer to Part 2.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
