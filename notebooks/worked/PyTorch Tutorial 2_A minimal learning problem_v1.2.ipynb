{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f518e2-5430-46bb-94f7-0bbb132a8c4c",
   "metadata": {},
   "source": [
    "# Scope of this notebook\n",
    "\n",
    "This notebook introduces a **minimal learning problem** in PyTorch, focusing on how gradient-based learning works at a mechanical level.  \n",
    "It demonstrates how model parameters are updated through repeated application of forward computation, loss evaluation, backpropagation, and parameter updates.\n",
    "\n",
    "The emphasis is on developing **clear intuition** for:\n",
    "- how losses are defined and computed,\n",
    "- how gradients are obtained using PyTorch’s automatic differentiation engine,\n",
    "- how parameter updates lead to learning over multiple iterations,\n",
    "- and how the learning rate affects convergence behaviour.\n",
    "\n",
    "All concepts are illustrated using explicit tensor operations and a fully manual training loop, without relying on higher-level abstractions.\n",
    "\n",
    "This notebook is intended as a **conceptual bridge** between basic tensor operations and more advanced PyTorch workflows.  \n",
    "Readers should finish this notebook with a solid understanding of *what learning means computationally*, before introducing optimisers or neural network modules.\n",
    "\n",
    "It does **not** introduce `nn.Module`, built-in optimisers, datasets, or data loaders.  \n",
    "These abstractions will be introduced in subsequent notebooks once the underlying mechanics are well understood.\n",
    "\n",
    "**Recommended prerequisites:** Familiarity with PyTorch tensor fundamentals, basic Python programming, and elementary linear algebra.\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Angze Li\n",
    "\n",
    "**Last updated:** 2026-02-03\n",
    "\n",
    "**Version:** v1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d26685db-2781-4046-88a8-b2c42d5ef99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad008c21-7229-439e-bd87-f464c7a4dd80",
   "metadata": {},
   "source": [
    "### What problem are we setting up?\n",
    "\n",
    "We consider a very small model with:\n",
    "- an input vector `x`\n",
    "- trainable parameters `w` and `b`\n",
    "- a loss function that measures how wrong the model output is\n",
    "\n",
    "The model produces an output by combining the input with its parameters:\n",
    "\n",
    "- `w` (the **weights**) determine how strongly each component of the input `x` influences the output.\n",
    "- `b` (the **bias**) provides an offset that allows the model to shift its output independently of the input.\n",
    "- Together, `w` and `b` define the behaviour of the model and are the quantities we want to learn.\n",
    "\n",
    "The **loss function** compares the model output to the expected output and returns a single scalar value that quantifies the error.  \n",
    "This scalar loss is what drives learning: by analysing how the loss changes with respect to `w` and `b`, we can adjust the parameters to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9737db3-2299-4668-8fe7-d1eae3163ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and target\n",
    "x = torch.ones(5)      # input features\n",
    "y = torch.zeros(3)    # expected output (target)\n",
    "\n",
    "# Trainable parameters\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584dbdf-cab7-4e65-9191-f63f87e22a18",
   "metadata": {},
   "source": [
    "### What these tensors represent\n",
    "- `x` is a fixed input vector.\n",
    "- `w` is a weight matrix mapping 5 inputs to 3 outputs.\n",
    "- `b` is a bias vector added to the output. (Note the dimensionalities of `x`, `w` and `b`.)\n",
    "- `requires_grad=True` tells PyTorch:\n",
    "  \n",
    "  > “Track how this tensor influences the final loss.”\n",
    "\n",
    "Only tensors with `requires_grad=True` will receive gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8ca49d8-d7cc-4b6c-aeaa-66e508e02fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output z: tensor([ 4.1986,  0.4618, -0.7181], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(\"Model output z:\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f6133-2872-4044-b2bf-e2ac9ee8ac69",
   "metadata": {},
   "source": [
    "### What just happened mathematically\n",
    "\n",
    "This line computes the **forward pass** of the model:\n",
    "\n",
    "$$\n",
    "z = x W + b\n",
    "$$\n",
    "\n",
    "- `x` is shape `(5,)`\n",
    "- `w` is shape `(5, 3)`\n",
    "- result `z` is shape `(3,)`\n",
    "\n",
    "Each element of `z` depends on **all entries of `w` and `b`**.\n",
    "\n",
    "PyTorch remembers this dependency internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a2a895f-737e-49a5-9f08-169b7dc6e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(1.8537, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349a5ac-eb4e-48ff-b388-02c4db1d6b1c",
   "metadata": {},
   "source": [
    "### Why we need a loss\n",
    "\n",
    "The loss is a **single scalar number** that measures how far the model output `z`\n",
    "is from the target `y`.\n",
    "\n",
    "In this example, we use **binary cross-entropy with logits** (hence the mame of the function `torch.nn.functional.binary_cross_entropy_with_logits()`) , which is computed *element-wise* as:\n",
    "\n",
    "$$\n",
    "\\ell(z_i, y_i) = - \\left[ y_i \\ln(\\sigma(z_i)) + (1 - y_i)\\ln(1 - \\sigma(z_i)) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function,\n",
    "- $z_i$ is the model output (logit) for the $i$-th entry,\n",
    "- $y_i \\in \\{0, 1\\}$ is the corresponding target.\n",
    "\n",
    "The total loss is obtained by averaging over all entries:\n",
    "\n",
    "$$\n",
    "\\text{loss} = \\frac{1}{N} \\sum_i \\ell(z_i, y_i)\n",
    "$$\n",
    "\n",
    "Key points:\n",
    "- The loss is a **scalar**, which is required for backpropagation.\n",
    "- Using logits (raw values `z`) instead of probabilities is **numerically more stable**.\n",
    "- PyTorch combines the sigmoid operation and cross-entropy computation internally.\n",
    "\n",
    "At this point, PyTorch has built a **computation graph** linking  \n",
    "`w, b → z → loss`.\n",
    "\n",
    "One can manually verify that the loss computed above follows this formula exactly by applying the sigmoid function to $z$ and evaluating the expression entry by entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8384c8-66aa-494e-a527-d94ca0144bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w before backward: None\n",
      "Gradient of b before backward: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient of w before backward:\", w.grad)\n",
    "print(\"Gradient of b before backward:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4be57-8e53-4ab7-8619-31e86c03a3c2",
   "metadata": {},
   "source": [
    "### Why gradients are `None`\n",
    "\n",
    "Gradients are not computed automatically.\n",
    "\n",
    "Up to now, we have only done a **forward pass**.\n",
    "No differentiation has happened yet.\n",
    "\n",
    "To compute gradients, we must explicitly ask PyTorch to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "600dca61-faef-4cad-ac6c-a7a7292b2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c362ead-8eec-4296-8a80-24763e5b1c57",
   "metadata": {},
   "source": [
    "### What `backward()` actually means\n",
    "\n",
    "Calling `loss.backward()` tells PyTorch to compute **how the loss changes with respect to each trainable parameter**.\n",
    "\n",
    "More concretely, PyTorch performs the following steps:\n",
    "\n",
    "1. **Start from the scalar `loss`**  \n",
    "   The loss is the final output of the computation graph. Gradients are always computed with respect to a scalar, because only then is the gradient well-defined.\n",
    "\n",
    "2. **Traverse the computation graph backwards**  \n",
    "   PyTorch follows the recorded operations in reverse order, moving from the loss back toward the parameters (`w` and `b`).\n",
    "\n",
    "3. **Apply the chain rule automatically**  \n",
    "   At each operation, PyTorch computes local derivatives and combines them using the chain rule, propagating gradients backward through the graph.\n",
    "\n",
    "4. **Accumulate gradients in leaf tensors**  \n",
    "   For every tensor marked with `requires_grad=True` that is a *leaf* of the graph (such as `w` and `b`), PyTorch stores the resulting gradients in the `.grad` attribute.\n",
    "\n",
    "After this process:\n",
    "- `w.grad` contains $\\partial\\ \\text{loss}/\\partial w$\n",
    "- `b.grad` contains $\\partial\\ \\text{loss}/\\partial b$\n",
    "\n",
    "This entire backward pass from the loss back to the parameters is known as **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b174b4-b096-42ca-b726-3a611a3657b5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Definition of leaf tensors</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Leaf tensors**:\n",
    "- are created directly by the user (or by detaching from a graph),\n",
    "- are **not** the result of any `autograd`-tracked operation,\n",
    "- are the tensors where gradients are **accumulated** during backpropagation.\n",
    "\n",
    "In our example so far, `w` and `b` are created directly using `torch.randn()` with `requires_grad=True`, and therefore they are **leaf tensors**.\n",
    "\n",
    "By contrast, `loss` is produced as the result of a sequence of operations (e.g., `binary_cross_entropy_with_logits`), so it is **not** a leaf tensor. Instead, it stores a `grad_fn` that defines how gradients should be propagated backward through the computation graph.\n",
    "\n",
    "You can check whether a tensor is a leaf tensor using:\n",
    "```python\n",
    "tensor.is_leaf\n",
    "```\n",
    "The result would be either `True` or `False`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10a4c464-17e8-472c-b168-e7c12fba6280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w:\n",
      " tensor([[0.3284, 0.2045, 0.1093],\n",
      "        [0.3284, 0.2045, 0.1093],\n",
      "        [0.3284, 0.2045, 0.1093],\n",
      "        [0.3284, 0.2045, 0.1093],\n",
      "        [0.3284, 0.2045, 0.1093]])\n",
      "Gradient of b:\n",
      " tensor([0.3284, 0.2045, 0.1093])\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient of w:\\n\", w.grad)\n",
    "print(\"Gradient of b:\\n\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b8c1f-cac5-4ece-a59f-5ce0a564bf9d",
   "metadata": {},
   "source": [
    "### How to interpret these gradients\n",
    "- `w.grad` tells us how the loss changes if each entry of `w` is *increased* slightly.\n",
    "- `b.grad` tells us how the loss changes if each entry of `b` is *increased* slightly.\n",
    "\n",
    "The gradients have the **same shape** as the parameters they belong to.\n",
    "This is not an accident — it allows **parameter-wise** updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc990303-fdc1-4b49-ac32-d2d52b50f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "    b -= learning_rate * b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177bf267-d5f3-46d5-80a6-5b926861c4b4",
   "metadata": {},
   "source": [
    "### Learning rate and `torch.no_grad()`\n",
    "\n",
    "The variable `learning_rate` controls **how large each parameter update is**.  \n",
    "It determines how far we move the parameters in the direction that reduces the loss.\n",
    "\n",
    "- A larger learning rate leads to **bigger steps**, which may speed up learning but risk overshooting.\n",
    "- A smaller learning rate leads to **smaller, more cautious steps**, which are more stable but slower.\n",
    "\n",
    "In this example, `learning_rate = 0.1` is chosen as a simple, reasonable value to clearly illustrate the update step.\n",
    "\n",
    "---\n",
    "By default, all tensors with `requires_grad=True` are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, because, e.g.:\n",
    "- Updating parameters is **not** part of the model itself, i.e., we only want to to forward computations through the network.\n",
    "- We do not want PyTorch to track these operations\n",
    "\n",
    "Otherwise, the computation graph would grow incorrectly.\n",
    "\n",
    "Expand the two cells below to learn more about learning rate and the necessity of `torch.no_grad()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2b982-acc3-4dde-bb76-77abc4a0e20c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Learning rate: scale, intuition, and practical behaviour</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### What is the usual range of the learning rate?\n",
    "\n",
    "There is no single “correct” learning rate, but in practice:\n",
    "\n",
    "- Common values range from **1e−4 to 1e−1**\n",
    "- Typical starting points are **0.1**, **0.01**, or **0.001**\n",
    "- The appropriate value depends on:\n",
    "  - the model,\n",
    "  - the loss function,\n",
    "  - and the scale of the gradients\n",
    "\n",
    "In this notebook, we use `learning_rate = 0.1` purely for illustration, not because it is optimal.\n",
    "\n",
    "---\n",
    "\n",
    "#### Intuition: the hill analogy\n",
    "\n",
    "Imagine the loss as a landscape with hills and valleys:\n",
    "\n",
    "- The **height** represents the loss value\n",
    "- The **slope** represents the gradient\n",
    "- The parameters (`w`, `b`) represent your current position on the hill\n",
    "\n",
    "The learning rate controls **how large a step you take downhill**:\n",
    "\n",
    "- Too small → you move very slowly and make little progress\n",
    "- Too large → you may overshoot the valley and oscillate\n",
    "- Just right → you move steadily toward a minimum\n",
    "\n",
    "---\n",
    "\n",
    "#### Why the learning rate matters\n",
    "\n",
    "The gradient tells you *which direction* reduces the loss.  \n",
    "The learning rate determines *how far* you move in that direction.\n",
    "\n",
    "Effective learning requires **both** a meaningful gradient and an appropriate learning rate.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014de5ed-f1f0-4a41-bbac-bcef3128db5d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Why <code>torch.no_grad()</code> is needed during parameter updates</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "We use `torch.no_grad()` because updating model parameters is **not part of the forward computation** that defines the model itself.\n",
    "\n",
    "During the forward pass, PyTorch records operations to build a computation graph.  \n",
    "However, parameter updates are a **bookkeeping step** performed after gradients have been computed.\n",
    "\n",
    "If parameter updates were performed without `torch.no_grad()`:\n",
    "- PyTorch would attempt to track these updates as part of the computation graph\n",
    "- The graph would incorrectly include the update operations themselves\n",
    "- Subsequent calls to `backward()` could fail or produce incorrect gradients\n",
    "\n",
    "Using `torch.no_grad()` temporarily disables gradient tracking, ensuring that:\n",
    "- parameter updates do not become part of the computation graph\n",
    "- each training step starts from a clean graph\n",
    "- gradients are computed only for the forward computation\n",
    "\n",
    "This separation between **model computation** and **parameter updates** is essential for correct and stable training.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eaab674-fbec-4f82-b462-4bbd7b56127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " tensor([[ 1.2187,  0.3929, -0.8412],\n",
      "        [-0.0269, -0.6704,  1.1079],\n",
      "        [ 0.5475, -0.9823, -0.0089],\n",
      "        [ 1.7217,  0.5403, -1.3044],\n",
      "        [-0.6329,  0.2808,  0.3156]], requires_grad=True)\n",
      "b:\n",
      " tensor([ 1.1734,  0.7778, -0.0526], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "print(\"w:\\n\", w)\n",
    "print(\"b:\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3636e4-b723-49ba-b472-1954cc187208",
   "metadata": {},
   "source": [
    "### Why gradients must be cleared\n",
    "\n",
    "By default, PyTorch **accumulates gradients**.\n",
    "\n",
    "This means:\n",
    "- Calling `backward()` multiple times without resetting gradients will **add new gradients** to the existing values stored in `.grad`\n",
    "- This behaviour is intentional and allows gradients to be accumulated across multiple backward passes (e.g. when summing losses)\n",
    "\n",
    "However, for most training loops—especially when learning—this accumulation can be confusing and lead to subtle bugs.\n",
    "\n",
    "Clearing gradients explicitly ensures that each backward pass computes gradients **only for the current forward computation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30798d17-b99e-4526-9672-7cda106dfdfc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a2516-51fd-4bb8-865d-63e34dc6c9f0",
   "metadata": {},
   "source": [
    "### Make it a loop\n",
    "\n",
    "So far, we have seen how to compute gradients and perform a **single parameter update**.\n",
    "\n",
    "Learning, however, does not happen in one step.  \n",
    "Instead, the process of:\n",
    "1. computing the loss,\n",
    "2. computing gradients via backpropagation,\n",
    "3. updating parameters,\n",
    "\n",
    "is repeated **many times**.\n",
    "\n",
    "Each repetition is called a **training step** (or iteration).  \n",
    "Over successive steps, the parameters are gradually adjusted to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48e1c3d0-100c-463b-86a3-e0ddb3e59553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0 | Loss: 1.757628\n",
      "Step  1 | Loss: 1.664862\n",
      "Step  2 | Loss: 1.575276\n",
      "Step  3 | Loss: 1.488735\n",
      "Step  4 | Loss: 1.405126\n",
      "Step  5 | Loss: 1.324370\n",
      "Step  6 | Loss: 1.246420\n",
      "Step  7 | Loss: 1.171264\n",
      "Step  8 | Loss: 1.098926\n",
      "Step  9 | Loss: 1.029459\n",
      "Step 10 | Loss: 0.962941\n",
      "Step 11 | Loss: 0.899468\n",
      "Step 12 | Loss: 0.839143\n",
      "Step 13 | Loss: 0.782065\n",
      "Step 14 | Loss: 0.728320\n",
      "Step 15 | Loss: 0.677965\n",
      "Step 16 | Loss: 0.631024\n",
      "Step 17 | Loss: 0.587479\n",
      "Step 18 | Loss: 0.547268\n",
      "Step 19 | Loss: 0.510287\n"
     ]
    }
   ],
   "source": [
    "# Simple training loop to demonstrate learning\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_steps = 20\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Forward pass\n",
    "    z = torch.matmul(x, w) + b\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameter update\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Clear gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # Monitor progress\n",
    "    print(f\"Step {step:2d} | Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60593910-18e6-49ad-99e3-89e1ed30d115",
   "metadata": {},
   "source": [
    "### The training loop: how learning emerges\n",
    "- Each loop iteration performs one **forward–backward–update** cycle.\n",
    "- The loss is recomputed using the updated parameters at every step.\n",
    "- If the learning rate is reasonable, the loss should gradually decrease.\n",
    "\n",
    "This loop demonstrates the core mechanism behind training neural networks:\n",
    "repeated application of backpropagation and parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b435b-1a03-45c0-9828-70e0f5a7d721",
   "metadata": {},
   "source": [
    "### What backpropagation really did here\n",
    "\n",
    "- We computed a forward pass to get a scalar loss\n",
    "- PyTorch recorded how that loss depends on parameters\n",
    "- `loss.backward()` computed all required gradients automatically\n",
    "- Gradients told us how to update parameters to reduce the loss\n",
    "\n",
    "No neural network magic — just calculus applied systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ac39cc-4195-4f74-9eab-98c0c45594a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260defd0-928d-4eff-9bac-c7c028642906",
   "metadata": {},
   "source": [
    "### Convergence in gradient-based learning\n",
    "\n",
    "At each iteration:\n",
    "1. A forward pass computes the model output and the loss.\n",
    "2. Backpropagation computes gradients of the loss with respect to the parameters.\n",
    "3. The parameters are updated in the direction that reduces the loss.\n",
    "4. Gradients are cleared in preparation for the next step.\n",
    "\n",
    "As the loop progresses, the loss stored in `loss_history` decreases and eventually falls below the chosen threshold.  \n",
    "This indicates that successive parameter updates are making **smaller and smaller improvements** (see the figure below for the visualisation of decrease in `loss`), and the model is approaching a minimum of the loss function.\n",
    "\n",
    "The stopping condition is:\n",
    "```python\n",
    "loss_value > 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef2b3054-0ed7-412f-bcc1-ab9dcbf567f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0 | Loss: 1.819560\n",
      "Step  100 | Loss: 0.059195\n",
      "Step  200 | Loss: 0.027554\n",
      "Step  300 | Loss: 0.017878\n",
      "Step  400 | Loss: 0.013214\n",
      "Step  500 | Loss: 0.010474\n",
      "\n",
      "Total learning steps: 524\n",
      "Final loss: 0.009996\n",
      "CPU times: user 35.7 ms, sys: 55.8 ms, total: 91.5 ms\n",
      "Wall time: 46.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3) \n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "loss_value = float(\"inf\")\n",
    "learning_step = 0\n",
    "max_steps = 1000\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "while loss_value > 0.01 and learning_step < max_steps:\n",
    "    # Forward pass\n",
    "    z = torch.matmul(x, w) + b\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameter update\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Clear gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # Extract scalar loss\n",
    "    loss_value = loss.item()\n",
    "    loss_history.append(loss_value)\n",
    "\n",
    "    # Monitor progress\n",
    "    if learning_step % 100 == 0:\n",
    "        print(f\"Step {learning_step:4d} | Loss: {loss_value:.6f}\")\n",
    "\n",
    "    learning_step += 1\n",
    "\n",
    "print(\"\\nTotal learning steps:\", learning_step)\n",
    "print(\"Final loss:\", round(loss_value, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60be9273-16b0-4fa6-866d-e1bb3bd1fd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss convergence over training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5ee24-6238-43e5-a5f6-e765cf113666",
   "metadata": {},
   "source": [
    "### The effect of learning rate\n",
    "The experiment below examines how the **learning rate** affects convergence in gradient-based learning.\n",
    "\n",
    "For each value in `learning_rates`, the model is re-initialised and trained under identical conditions. This ensures that any differences in behaviour arise solely from the choice of learning rate, rather than from initialisation or model structure.\n",
    "\n",
    "The reported results show how many steps are required for the loss to fall below the threshold\n",
    "```python\n",
    "loss_value > 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "776c10ce-10d8-4aaa-8433-a624d48b82ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate = 0.001 | steps = 1000 | final loss = 0.295069\n",
      "learning_rate = 0.01  | steps = 1000 | final loss = 0.056271\n",
      "learning_rate = 0.1   | steps = 409  | final loss = 0.009993\n",
      "learning_rate = 1.0   | steps = 48   | final loss = 0.009893\n",
      "learning_rate = 10    | steps = 2    | final loss = 0.007733\n",
      "learning_rate = 100   | steps = 2    | final loss = 0.000005\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "max_steps = 1000\n",
    "threshold = 0.01\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Re-initialise model for fair comparison\n",
    "    x = torch.ones(5)\n",
    "    y = torch.zeros(3) \n",
    "    w = torch.randn(5, 3, requires_grad=True)\n",
    "    b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "    loss_value = float(\"inf\")\n",
    "    learning_step = 0\n",
    "    loss_history = []\n",
    "\n",
    "    while loss_value > threshold and learning_step < max_steps:\n",
    "        # Forward pass\n",
    "        z = torch.matmul(x, w) + b\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameter update\n",
    "        with torch.no_grad():\n",
    "            w -= lr * w.grad\n",
    "            b -= lr * b.grad\n",
    "\n",
    "        # Clear gradients\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "        # Extract scalar loss\n",
    "        loss_value = loss.item()\n",
    "        loss_history.append(loss_value)\n",
    "\n",
    "        learning_step += 1\n",
    "\n",
    "    print(f\"learning_rate = {lr:<5} | steps = {learning_step:<4} | final loss = {loss_value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862089a-094c-4a2a-8e06-3c42d28a4e1b",
   "metadata": {},
   "source": [
    "### Interpreting learning rate effects\n",
    "\n",
    "In this simple and well-conditioned example, larger learning rates reduce the number of steps required for convergence, allowing the loss to fall below the chosen threshold more quickly.\n",
    "\n",
    "However, this behaviour does not generalise to all optimisation problems.  In more complex loss landscapes, overly large learning rates can lead to:\n",
    "- unstable updates,\n",
    "- divergence of the loss,\n",
    "- or convergence to poor solutions that generalise poorly.\n",
    "\n",
    "For this reason, smaller learning rates are often preferred in practice, as they provide greater **robustness and stability** across a wide range of models and training conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33f4f1-d9a7-4315-82cd-9b477a7e2e18",
   "metadata": {},
   "source": [
    "### Extra: Inspecting the computation graph with `grad_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8344547-26a9-467b-a15d-d14c690d009d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.grad_fn: <AddBackward0 object at 0x32d6da620>\n",
      "w.grad_fn: None\n",
      "b.grad_fn: None\n",
      "z.grad_fn after detach(): None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5)\n",
    "y = torch.zeros(3) \n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "z = torch.matmul(x, w) + b\n",
    "print(\"z.grad_fn:\", z.grad_fn)\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "    b -= learning_rate * b.grad\n",
    "\n",
    "print(\"w.grad_fn:\", w.grad_fn)\n",
    "print(\"b.grad_fn:\", b.grad_fn)\n",
    "\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "loss_value = loss.item()\n",
    "\n",
    "z = z.detach()\n",
    "print(\"z.grad_fn after detach():\", z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68bae0-798e-4682-bae6-7c2b330bea90",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>How computation graphs track and control gradient flow</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the code above, we observe the following:\n",
    "\n",
    "```text\n",
    "z.grad_fn: <AddBackward0 object at 0x...>\n",
    "w.grad_fn: None\n",
    "b.grad_fn: None\n",
    "```\n",
    "#### Why does `z.grad_fn` exist, but `w.grad_fn` and `b.grad_fn` do not?\n",
    "The tensor `z` is computed as the result of operations:\n",
    "\n",
    "```python\n",
    "z = torch.matmul(x, w) + b\n",
    "```\n",
    "\n",
    "Because `z` is produced by operations, PyTorch attaches a **gradient function** to it.\n",
    "This function, accessible via `z.grad_fn`, records how `z` was computed and how gradients should be propagated backward during backpropagation.\n",
    "\n",
    "By contrast, `w` and `b` are **leaf tensors** and therefore have `grad_fn = None`, even though they still receive gradients via their `.grad` attributes after calling `backward()`.\n",
    "\n",
    "#### Breaking the computation graph with `detach()`\n",
    "The method `detach()` creates a new tensor that shares the same underlying data but is disconnected from the computation graph.\n",
    "\n",
    "In this example, we explicitly overwrite `z`:\n",
    "```python\n",
    "z = z.detach()\n",
    "```\n",
    "After detaching:\n",
    "- `z` is treated as a constant,\n",
    "- no gradients will flow backward through it,\n",
    "- and operations involving `z` will not be tracked by autograd.\n",
    "This is useful when you intentionally want to stop gradient flow or avoid growing the computation graph.\n",
    "\n",
    "#### What does <AddBackward0 object at 0x...> mean?\n",
    "The object `<AddBackward0>` represents the backward rule for the addition operation that produced `z`.\n",
    "\n",
    "In this example, the final operation was:\n",
    "```Python\n",
    "z = (torch.matmul(x, w)) + b\n",
    "```\n",
    "PyTorch therefore attaches an **AddBackward function** to `z`, which knows how to:\n",
    "- apply the chain rule,\n",
    "- and propagate gradients from `z` to its inputs during backpropagation.\n",
    "\n",
    "The hexadecimal value (e.g., 0x32d6da620) is simply the memory address of this Python object during the current execution.\n",
    "\n",
    "It has no mathematical meaning and can be safely ignored.\n",
    "\n",
    "There are many other functions besides `<AddBackward0>`.\n",
    "\n",
    "Each differentiable operation (e.g. matrix multiplication, activation functions, loss functions) has its own corresponding backward function, which appears as the `grad_fn` of the resulting tensor. For example, the tensor loss has a different `grad_fn` reflecting the loss operation used to compute it.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4becea22-e217-46d6-b30f-315e29a019f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.grad_fn: <BinaryCrossEntropyWithLogitsBackward0 object at 0x32d672f80>\n"
     ]
    }
   ],
   "source": [
    "print(\"loss.grad_fn:\", loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ea2f0-60ef-41c3-8124-7e296a740776",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section, we examined a minimal learning problem and followed the entire training process step by step.\n",
    "\n",
    "Starting from a simple linear model, we:\n",
    "- defined a loss function and evaluated model performance,\n",
    "- used PyTorch’s automatic differentiation engine to compute gradients via backpropagation,\n",
    "- updated parameters explicitly using gradient descent,\n",
    "- and observed convergence over multiple training iterations under different learning rates.\n",
    "\n",
    "Along the way, we also explored how PyTorch represents computations internally using **computation graphs**, including the role of `grad_fn`, leaf tensors, and how gradient flow can be intentionally controlled using `detach()`.\n",
    "\n",
    "This example captures the essential mechanics underlying gradient-based learning, without relying on higher-level abstractions.\n",
    "\n",
    "In the next sections, these same ideas will be reused and extended using higher-level PyTorch constructs such as modules, optimisers, and more complex models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
