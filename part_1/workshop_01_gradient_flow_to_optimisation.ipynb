{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec51929b-f6c6-49a0-b2d0-833433114aeb",
   "metadata": {},
   "source": [
    "# ðŸ§ª Workshop 1: From Gradient Flow to Optimisation Intuition\n",
    "> This notebook is provided in a clean, non-executed version for the reader to try out on the problem.\n",
    ">\n",
    "> A model answer and executive summary can be found in the *worked* version.\n",
    "\n",
    "This set of workshops builds on Tutorials 1â€“4 and closes **Part 1** of the series, consolidating core ideas about PyTorch `autograd` and gradient flow before moving on to explicit optimisation in and Part 2.\n",
    "\n",
    "Rather than treating gradients as black-box training signals, the workshop emphasises **gradients as sensitivity measures**, especially in settings where model outputs are **tensor-valued** and gradients are computed using **explicit upstream directions**.\n",
    "\n",
    "---\n",
    "**The problems are designed to shift perspective**:\n",
    "- from *â€œwhat gradient does PyTorch give me?â€*\n",
    "- to *â€œwhat does this gradient represent, and why?â€*\n",
    "---\n",
    "**The emphasis is on developing intuition for**:\n",
    "- how gradients flow through structured linear and nonlinear computations,\n",
    "- how upstream gradients select directions in output space,\n",
    "- how `.grad` reflects sensitivity rather than optimisation,\n",
    "- and how gradient structure anticipates optimisation behaviour.\n",
    "---\n",
    "**Key ideas explored include**:\n",
    "- vectorâ€“Jacobian products as the fundamental object computed by `backward(v)`,\n",
    "- the role of upstream gradients in shaping gradient flow,\n",
    "- sparsity and structure in gradients induced by linear maps and nonlinearities (e.g., ReLU),\n",
    "- interpreting gradients statistically and geometrically rather than procedurally,\n",
    "- and using controlled experiments to reason about gradient behaviour.\n",
    "---\n",
    "**This workshop serves as a conceptual bridge between**:\n",
    "- `autograd` mechanics and gradient flow (Tutorials 3â€“4),\n",
    "- and **objective design and optimisation dynamics** (Workshop 2 and Part 2).\n",
    "\n",
    "The focus is intentionally *not* on training pipelines, datasets, or optimisers, but on understanding how gradients behave in controlled settingsâ€”laying the groundwork for effective optimisation.\n",
    "\n",
    "---\n",
    "**Recommended prerequisites**:\n",
    "- Familiarity with PyTorch tensors and `.backward()`\n",
    "- Understanding of scalar vs tensor-valued gradients\n",
    "- Basic comfort with linear algebra and nonlinear activations\n",
    "\n",
    "---\n",
    "\n",
    "**Author: Angze Li**\n",
    "\n",
    "**Last updated: 2026-02-19**\n",
    "\n",
    "**Version: v1.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a2f5c-2c82-4a64-9514-14869acffc75",
   "metadata": {},
   "source": [
    "## ðŸ§© Problem: Sensitivity of a Feature Transformation\n",
    "\n",
    ">In many models, inputs are transformed into feature representations before being used by a downstream objective.\n",
    "> Understanding which inputs influence which features is critical for interpretability and optimisation.\n",
    "\n",
    "Consider the following setup:\n",
    "```python\n",
    "x = torch.randn(6, requires_grad=True)\n",
    "\n",
    "W = torch.tensor([\n",
    "    [1.0,  0.0,  0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0,  1.0,  0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0,  0.0,  2.0, 0.0, 0.0, 0.0],\n",
    "    [0.0,  0.0,  0.0, 3.0, 0.0, 0.0],\n",
    "], requires_grad=False)\n",
    "\n",
    "out = torch.relu(W @ x) # what does this line do?\n",
    "```\n",
    "This produces a 4-dimensional tensor output, not a scalar.\n",
    "\n",
    "---\n",
    "\n",
    "### Task\n",
    "1.\tConstruct an upstream gradient vector `v` such that:\n",
    "    - the last feature of out is weighted most heavily,\n",
    "    - the first two features are ignored.\n",
    "2. Call:\n",
    "```python\n",
    "out.backward(v)\n",
    "```\n",
    "3. Inspect `x.grad`.\n",
    "\n",
    "---\n",
    "\n",
    "### Questions to think about\n",
    "- Which components of `x` receive non-zero gradients?\n",
    "- How does the structure of `W` affect gradient flow?\n",
    "- How does ReLU change which inputs are â€œactiveâ€?\n",
    "- Why is `x.grad` sparse in some cases?\n",
    "\n",
    "---\n",
    "\n",
    "### Hint\n",
    "\n",
    "> Think in terms of which inputs influence which outputs, and which outputs are being emphasised by `v`.\n",
    "\n",
    "---\n",
    "\n",
    "### Learning outcomes\n",
    "\n",
    "After this problem, you should be comfortable with:\n",
    "- interpreting `.grad` as a sensitivity map,\n",
    "- reasoning about gradient flow through linear + nonlinear layers,\n",
    "- understanding how upstream weighting reshapes gradient influence.\n",
    "\n",
    "This problem bridges Tutorials 3 and 4 cleanly and warms up the optimisation mindset.\n",
    "\n",
    "---\n",
    "**Author**: Angze Li\n",
    "\n",
    "**Last updated**: 2026-02-20\n",
    "\n",
    "**Version**: v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be29b7c-684b-4d89-89ea-e7f082b621fd",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36961af5-9adf-4ef2-b858-f8058d83481b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d692fc96-5526-4ac8-8b69-805bb671d3c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Executive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f7bdd-dea7-43c0-954b-0d4d40b055f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Understanding the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9bd9b-3d6c-47e3-9286-dcfb9095f037",
   "metadata": {},
   "source": [
    "This exercise explores how gradients flow through a simple linearâ€“nonlinear system when the output is tensor-valued and gradients are computed via a vectorâ€“Jacobian product.\n",
    "\n",
    "The model we studied is:\n",
    "$$\\text{out} = \\operatorname{ReLU}(W x),$$\n",
    "with a user-chosen upstream gradient `v` supplied to `backward()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cabbd3d-bbb6-4c4c-baa4-bdfffcbf31df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Which components of `x` receive non-zero gradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66555f0-b84f-4ecb-abf7-b1c75eb403e2",
   "metadata": {},
   "source": [
    "Only a **subset of the components of `x`** ever receive non-zero gradients.\n",
    "\n",
    "This happens for two reasons:\n",
    "1. **Linear structure**: Each row of `W` depends only on one component of `x.\n",
    "\n",
    "    As a result, each output entry is influenced by exactly one input coordinate.\n",
    "2. **Upstream weighting**: Only the output components with non-zero entries in `v` contribute to the gradient.\n",
    "\n",
    "If a component of `x` does not influence any weighted output entry, its gradient is exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b884eec-57a5-4d7e-b3e6-b8751800df45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### How does the structure of `W` affect gradient flow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a0d0be-ee68-456f-827a-0a7a9cfc2a51",
   "metadata": {},
   "source": [
    "The matrix `W` acts as a **routing mechanism** for gradients.\n",
    "\n",
    "Because `W` is sparse and diagonal-like:\n",
    "- each output dimension maps directly to a single input dimension,\n",
    "- gradient flow is strictly localised,\n",
    "- there is no mixing between unrelated components of `x`.\n",
    "\n",
    "This makes the gradient behaviour easy to interpret:\n",
    "changing the weight of one output only affects the gradient of its corresponding input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f72774-d7d3-4264-ad7d-7ca7517d6bfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### How does ReLU change which inputs are â€œactiveâ€?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f828e2-27b5-4133-9b19-2df96cf36d80",
   "metadata": {},
   "source": [
    "ReLU introduces **input-dependent gating**.\n",
    "\n",
    "For each output component:\n",
    "$$\n",
    "\\operatorname{ReLU}(z) =\n",
    "\\begin{cases}\n",
    "z, & z > 0 \\\\\n",
    "0, & z \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- gradients flow **only when the pre-activation is positive**,\n",
    "- inputs corresponding to negative pre-activations are *temporarily invisible* to backpropagation.\n",
    "\n",
    "In this exercise, averaging gradients over many random samples reveals that:\n",
    "- each active ReLU contributes gradients roughly **half** the time,\n",
    "- inactive paths contribute nothing.\n",
    "\n",
    "This explains the appearance of **fractional average gradients** in the Monte Carlo results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266f18e-8997-4e66-8d75-9ca6036d008f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Why is `x.grad` sparse in some cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b47cf8-3882-4779-ac8c-6d13ed21771a",
   "metadata": {},
   "source": [
    "`x.grad` is sparse because gradient flow is conditional:\n",
    "- conditional on which outputs are weighted by `v`,\n",
    "- conditional on which ReLU units are active,\n",
    "- conditional on the sparsity pattern of `W`.\n",
    "\n",
    "If any link in this chain is broken (zero weight, inactive ReLU, or missing linear connection), the gradient for that input vanishes.\n",
    "\n",
    "This sparsity is not a numerical artifact â€” it is a structural property of the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8852a-35d8-4334-b5ce-697c811d7720",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Why are the figures necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca161468-a306-4089-9778-4be9f1ab5133",
   "metadata": {},
   "source": [
    "The figures serve two essential purposes.\n",
    "\n",
    "**1. Revealing how upstream weights redistribute gradients**\n",
    "\n",
    "The first figure shows how changing the upstream weights `v` **redistributes gradient magnitude** across different input components.\n",
    "\n",
    "While the total sensitivity remains bounded, its allocation shifts smoothly as `v` changes.\n",
    "This visually reinforces the idea that:\n",
    "\n",
    "> vectorâ€“Jacobian products do not create new gradients â€” they reweight existing ones.\n",
    "\n",
    "**2. Confirming a linear relationship between gradient ratios and weight ratios**\n",
    "\n",
    "The second figure plots:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_3}{\\partial x_2}\n",
    "\\quad \\text{vs.} \\quad\n",
    "\\frac{v_3}{v_2}\n",
    "$$\n",
    "The near-perfect linear relationship confirms that:\n",
    "- gradient ratios scale linearly with upstream weight ratios,\n",
    "- the computation behaves exactly as predicted by the chain rule,\n",
    "- ReLUâ€™s gating only affects *when* gradients flow, not *how* they scale once active.\n",
    "\n",
    "This provides strong empirical validation of the theory behind vectorâ€“Jacobian products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f586c63-031e-4361-913d-3883c486dc69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ§­ Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e15cb78-175f-48cc-b3d8-6e0d9803c1cc",
   "metadata": {},
   "source": [
    "This exercise demonstrates that in PyTorch:\n",
    "- gradients of tensor-valued outputs are **not single objects**, but **directional sensitivities**,\n",
    "- `backward(v)` computes how a *chosen direction in output space* propagates back to inputs,\n",
    "- sparsity and structure in gradients arise naturally from the computation graph itself.\n",
    "\n",
    "Together, the code and figures make vectorâ€“Jacobian products concrete, intuitive, and experimentally verifiable â€” exactly the conceptual bridge needed before moving on to optimisation in Part 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
