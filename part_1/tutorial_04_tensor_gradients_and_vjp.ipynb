{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f8e9b0-5c07-4952-b9da-dd8e2f74699a",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Tutorial 4: Tensor Gradients and Vectorâ€“Jacobian Products\n",
    "> This notebook is provided in a clean, non-executed state for readability and reproducibility.\n",
    ">\n",
    "> A model answer and executive summary for the practical exercise can be found in the *worked* version.\n",
    "\n",
    "This tutorial builds on Tutorials 1â€“3 and deepens the understanding of PyTorch autograd by moving beyond scalar losses to tensor-valued outputs.\n",
    "\n",
    "In many realistic settings, computations do not naturally terminate in a single scalar. When the output is a tensor, the notion of â€œthe gradientâ€ must be refined. This tutorial introduces **vectorâ€“Jacobian products (VJPs)** as the fundamental object computed by `autograd` when calling `backward()` on non-scalar outputs.\n",
    "\n",
    "---\n",
    "**Rather than treating `backward()` as a black-box operation, the tutorial reframes it as a precise mathematical request:**\n",
    "- not *â€œgive me the gradientâ€*,\n",
    "- but *â€œhow does a weighted combination of outputs depend on earlier tensors?â€*\n",
    "---\n",
    "**The emphasis is on developing intuition for**:\n",
    "- how gradients are defined for tensor-valued functions,\n",
    "- why full Jacobians are rarely constructed explicitly,\n",
    "- how explicit upstream gradients select directions in output space,\n",
    "- and how `.grad` should be interpreted as a **sensitivity measure**, not a loss signal.\n",
    "---\n",
    "**Key ideas explored include**:\n",
    "- the distinction between scalar losses and tensor-valued outputs,\n",
    "- vectorâ€“Jacobian products as the object computed by `backward(v)`,\n",
    "- the role of upstream gradients in shaping gradient flow,\n",
    "- gradient accumulation and the need for explicit zeroing,\n",
    "- and interpreting gradient structure through controlled visual experiments.\n",
    "---\n",
    "**This tutorial serves as a conceptual bridge between**:\n",
    "- computation graph mechanics and backpropagation (Tutorial 3),\n",
    "- and gradient interpretation and objective design (Workshop 1).\n",
    "\n",
    "The focus is intentionally not on optimisation algorithms or training loops, but on **understanding what gradients mean** before they are used to update parameters.\n",
    "\n",
    "---\n",
    "**Recommended prerequisites**:\n",
    "- Familiarity with PyTorch tensors and `.backward()`\n",
    "- Understanding of computation graphs and leaf tensors\n",
    "- Basic comfort with linear algebra and partial derivatives\n",
    "---\n",
    "**Author**: Angze Li\n",
    "\n",
    "**Last updated**: 2026-02-20\n",
    "\n",
    "**Version**: v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867e6a6-d0d5-42da-ac1e-5cb9488ba629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28fc4c4-7925-4ce2-a027-fcf793502f52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. From scalar losses to tensor-valued outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d232390-ec96-4acf-8e15-2d5dd931815b",
   "metadata": {},
   "source": [
    "In most machine learning workflows, we work with a scalar loss function and compute gradients with respect to model parameters.\n",
    "This scalar setting allows PyTorch to apply backpropagation automatically, storing gradients directly in `.grad`.\n",
    "\n",
    "However, not all computations naturally end in a scalar.\n",
    "\n",
    "In many practical and analytical settings, the output of interest is an arbitrary **tensor** rather than a single number. In such cases, a full gradient is no longer *well-defined* in the usual sense. Instead, the derivative of a tensor-valued function is a **Jacobian matrix**, which can be large and expensive to construct explicitly.\n",
    "\n",
    "Rather than forming this Jacobian, PyTorch computes Jacobian products efficiently.\n",
    "\n",
    "Specifically, PyTorch evaluates **vectorâ€“Jacobian products** (VJPs), which answer the question:\n",
    "\n",
    "> How does a weighted combination of the output change with respect to earlier tensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b3581-abd5-4139-8066-c7f9b10d50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp+1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"leaf tensor inp\\n{inp}\")\n",
    "print(f\"out\\n{out}\")\n",
    "print(f\"First call\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e439d-a8eb-406e-934a-7dd9ec00b746",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What does the code mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864bfc4-f8bf-4e3d-bf01-48fe32ac3da5",
   "metadata": {},
   "source": [
    "#### First Line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193534aa-041f-488d-8df6-533683d6a16a",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.eye()\n",
    "```\n",
    "creates a identity-like matrix (ones on the diagonal, zeros elsewhere). \n",
    "\n",
    "By passing `requires_grad=True`, we tell PyTorch to track all subsequent operations on this tensor so that gradients can be computed during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304144b-724d-4b9a-b578-d47daecbb9c8",
   "metadata": {},
   "source": [
    "#### Second Line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8466c12-8e22-4389-9e6d-72228b5a7c6b",
   "metadata": {},
   "source": [
    "The variable `out` is now a **tensor-valued output**, not a scalar:\n",
    "```python\n",
    "out = (inp+1).pow(2).t()\n",
    "```\n",
    "This line performs several operations in sequence:\n",
    "1. `inp + 1` adds 1 element-wise to the input tensor.\n",
    "2. `.pow(2)` squares each element, introducing a nonlinear operation (common in machine learning models).\n",
    "3. `.t()` transposes the matrix, changing its shape.\n",
    "\n",
    "As a result, out is a matrix, not a single number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b2e55-130d-47f2-accf-16017153fa04",
   "metadata": {},
   "source": [
    "#### Third Line: why does `backward()` need an argument here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e159fc-8c16-4f1a-aff0-f0e4ad1917b6",
   "metadata": {},
   "source": [
    "In earlier tutorials, we always called backward() on a scalar loss. In that case, PyTorch *implicitly* assumes an upstream gradient of 1.\n",
    "\n",
    "Here, however, out is not a scalar. Calling:\n",
    "```python\n",
    "out.backward()\n",
    "```\n",
    "would raise an error, because PyTorch does not know which **combination of gradients** you want.\n",
    "\n",
    "Instead, we must explicitly provide an upstream gradient with the **same shape** as `out`:\n",
    "```python\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "```\n",
    "This tells PyTorch:\n",
    "> â€œCompute the vectorâ€“Jacobian product where the upstream gradient is a tensor of ones.â€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59983c-a885-49f1-a95f-2d4c473ca3e7",
   "metadata": {},
   "source": [
    "#### Final line: What is `inp.grad` showing us?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d195f-bb5a-490d-97b6-c7bb5e3ee6ab",
   "metadata": {},
   "source": [
    "After calling:\n",
    "```python\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "```\n",
    "PyTorch computes gradients **with respect to all leaf tensors** that have `requires_grad=True`.\n",
    "In this example, the only such tensor is:\n",
    "```python\n",
    "inp\n",
    "```\n",
    "As a result, PyTorch stores the gradient of the output `out` with respect to `inp` in:\n",
    "```python\n",
    "inp.grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979b20c-08a8-4b19-88d3-a266ec49bfe1",
   "metadata": {},
   "source": [
    "#### Wait a second..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceff67a-c095-4c9b-a5f5-56fdf06300ae",
   "metadata": {},
   "source": [
    "This could be counter-intuitive for beginners, as the `backward()` is called on `out`, but the gradient is stored on `inp`. \n",
    "\n",
    "However, the key mental shift is:\n",
    "> `backward()` is not asking â€œwhat is the gradient of this tensor?â€\n",
    ">\n",
    "> It is asking â€œhow did this output depend on earlier tensors?â€\n",
    "\n",
    "In PyTorch, when you call:\n",
    "```python\n",
    "out.backward(torch.ones_like(out))\n",
    "```\n",
    "you are telling PyTorch:\n",
    "> â€œAssume out represents the end of a computation.\n",
    "> \n",
    ">Please compute how changes in earlier tensors would affect it.â€\n",
    "\n",
    "More precisely, PyTorch computes:\n",
    "$$\\frac{\\partial}{\\partial (\\text{leaf tensors})}\n",
    "\\Big(v_{\\text{upstream}}^{\\top}\\cdot\\text{out}\\Big)$$\n",
    "where:\n",
    "- $v_{\\text{upstream}}$ is the upstream gradient supplied to `backward()` (in our case, `torch.ones_like(out)`, which corresponds to summing all entries of out with **equal weight**; other choices are also valid).\n",
    "- The expression $v_{\\text{upstream}}^{\\top}\\cdot\\text{out}$ is equivalent to $\\sum_i v_{\\text{upstream},i}\\,\\text{out}_i$, giving a scalar as the result. This is called **gradient produced via a vectorâ€“Jacobian product (VJP)**.\n",
    "- Only the leaf tensors (see definition in Tutorial 2) accumulate gradients in their `.grad` attribute during backpropagation (in our case, `inp` is the (only) leaf tensor, and hence PyTorch stores the result of the above derivative in `inp.grad`).\n",
    "\n",
    "Note that mathematically, the gradient (partial derivatives) of a scalar $L$ with respect to a matrix $X \\in \\mathbb{R}^{m\\times n}$ is defined as:\n",
    "$$\\frac{\\partial L}{\\partial X}\n",
    "\\;=\\;\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial X_{11}} & \\cdots & \\frac{\\partial L}{\\partial X_{1n}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial X_{m1}} & \\cdots & \\frac{\\partial L}{\\partial X_{mn}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "There are (at least) two reasons why PyTorch does not store the gradient in `out`:\n",
    "1. **`out` is not a parameter.** We donâ€™t usually update out; we update inputs like weights.\n",
    "2. **Gradients are defined with respect to inputs.** The whole purpose of backpropagation is to answer: â€œWhich earlier values should I change to reduce the output?â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56453a7-0082-4746-b705-31257daa69d5",
   "metadata": {},
   "source": [
    "#### Important clarification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e46e5-ed45-49ec-ba1e-7597fb2a500b",
   "metadata": {},
   "source": [
    "The need to pass an argument to `backward()` here is not because we want to call `backward()` multiple times. It is because `out` is non-scalar.\n",
    "\n",
    "The reason we pass `retain_graph=True` is separate:\n",
    "- it allows the same computation graph to be reused in later cells,\n",
    "- which is useful for demonstration and debugging.\n",
    "\n",
    "We will discuss gradient accumulation and zeroing gradients in the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff58bcf-3625-4c38-a4b9-b8471dda0d4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Gradient accumulation and zeroing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b64e1-7d08-460e-9ca7-61e939872982",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "\n",
    "if inp.grad is not None:\n",
    "    inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896986bb-4965-4420-8791-5d63cddcf965",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What is happening here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10cf30-b908-4e0a-98e4-39778c892c39",
   "metadata": {},
   "source": [
    "PyTorch accumulates gradients by default.\n",
    "This means that every call to `backward()` adds new gradient contributions to the existing values stored in `.grad`.\n",
    "\n",
    "When we call `backward()` for a second time:\n",
    "```python\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "```\n",
    "without clearing gradients:\n",
    "- PyTorch computes the same gradient again\n",
    "- the newly computed gradient is **added to the existing contents** of `inp.grad`\n",
    "\n",
    "As a result, `inp.grad` after the second call contains the **sum of gradients** from both backward passes, not just the most recent one (we can find that the result of second call is exactly twice as the first call above).\n",
    "\n",
    "This behaviour is intentional and is essential for many training workflows (e.g. gradient accumulation over multiple mini-batches).\n",
    "\n",
    "However, when we intend to reset the gradients, we can use:\n",
    "```python\n",
    "inp.grad.zero_()\n",
    "```\n",
    "This clears the accumulated gradient and sets all entries of `inp.grad` to zero. Now, when we call\n",
    "```python\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "```\n",
    "again:\n",
    "- PyTorch computes the gradient from scratch\n",
    "- the result stored in `inp.grad` now reflects only this single backward pass.\n",
    "\n",
    "Now the result is the same as the first call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123bf6b-b7cf-4b4b-962f-03af16860935",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Why does PyTorch accumulate gradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca4bd7-73eb-4369-82dc-363fe350badc",
   "metadata": {},
   "source": [
    "PyTorchâ€™s design assumes that:\n",
    "- users may want to accumulate gradients across multiple forwardâ€“backward passes,\n",
    "- or combine gradients from multiple loss terms.\n",
    "\n",
    "Because of this, gradient clearing is an **explicit** user responsibility.\n",
    "\n",
    "This is why, in typical training loops, you will often see:\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "or \n",
    "```python\n",
    "tensor.grad.zero_()\n",
    "```\n",
    "before each backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc734004-89ba-489b-9e64-e061f1ed25a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.  Practical exercise: weighted sensitivity of a tensor output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b920fa8-c9f0-4be2-82cd-b017cf24e792",
   "metadata": {},
   "source": [
    ">In many real problems, a model does not produce a single scalar output.\n",
    ">Instead, we often care about how sensitive different parts of the output are to changes in the input, possibly with different importance weights.\n",
    "\n",
    "Consider the following setup:\n",
    "```python\n",
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp + 10).pow(3).t()\n",
    "```\n",
    "This produces a tensor-valued output.\n",
    "Suppose now that we care more about *some entries* of `out` than others.\n",
    "\n",
    "---\n",
    "#### Task\n",
    "\n",
    "1. Construct an upstream gradient tensor v with the same shape as out that:\n",
    "    - assigns higher weight to the first row of out,\n",
    "    - and lower (or zero) weight elsewhere.\n",
    "2.\tCall:\n",
    "```python\n",
    "out.backward(v)\n",
    "```\n",
    "3.\tInspect `inp.grad`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Questions to think about\n",
    "- How does changing `v` change `inp.grad`?\n",
    "- Which entries of `inp` are most sensitive to the weighted output?\n",
    "- What does `inp.grad` represent conceptually in this case?\n",
    "\n",
    "---\n",
    "\n",
    "#### Hint\n",
    "\n",
    ">You are not computing â€œthe gradient of outâ€.\n",
    ">\n",
    ">You are computing how a weighted combination of outputs depends on the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b06ad-3389-4154-a896-f5c9335b54fc",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5e7f7-82d4-4c8d-b2a0-c571b5288cda",
   "metadata": {},
   "source": [
    "## ðŸ§­ Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a35e4-38aa-43dd-90b2-c40b057a23a5",
   "metadata": {},
   "source": [
    "This tutorial completes the transition from scalar losses to tensor-valued outputs, clarifying whatÂ `autograd`Â computes when a computation does not naturally end in a single number.\n",
    "\n",
    "By working explicitly with vectorâ€“Jacobian products, you have seen that gradients are not intrinsic properties of tensors, but **responses to a chosen direction in output space**. The upstream gradient supplied toÂ `backward(v)`Â is not a technical nuisanceâ€”it is a precise way of defining *what you care about*.\n",
    "\n",
    "This perspective is essential before moving on to optimisation:\n",
    "- it explains why gradients depend on objectives, not just models,\n",
    "- it clarifies how sensitivity is redistributed across inputs,\n",
    "- and it demystifies the role ofÂ `.grad`Â as a geometric object rather than a training artifact.\n",
    "\n",
    "In the workshops, these ideas will be pushed further. Instead of merely inspecting gradients, you will begin to **shape them deliberately**Â and interpret how their structure anticipates optimisation dynamicsâ€”without yet introducing optimisers or training loops.\n",
    "\n",
    "At that point, optimisation will no longer feel like a new concept, but simply the repeated application of principles you already understand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
