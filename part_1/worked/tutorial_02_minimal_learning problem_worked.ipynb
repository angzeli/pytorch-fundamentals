{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f518e2-5430-46bb-94f7-0bbb132a8c4c",
   "metadata": {},
   "source": [
    "# üìò Tutorial 2: A Minimal Learning Problem and First Contact with Autograd\n",
    "\n",
    "This tutorial builds on the tensor fundamentals introduced in Tutorial 1 and provdies the **first concrete encounter with learning and optimisation** in PyTorch.\n",
    "\n",
    "Rather than starting with large models or complex architectures, the tutorials focuses on a **deliberately minimal learning problem**, allowing every component of the training process to be examined in isolation and understood mechanistically.\n",
    "\n",
    "The goal is not performance, but clarity.\n",
    "\n",
    "---\n",
    "**The tutorial is designed to shift perspective**:\n",
    "\n",
    "The tutorial is designed to move the reader:\n",
    "- from *\"tensors as static numerical objects\"*\n",
    "- to *\"tensors as quantities connected by a computation graph\"*\n",
    "\n",
    "In particular, it introduces gradients not as abstract symbols, but as **quantities produced by backpropogation** and stored on leaf tensors via `.grad`.\n",
    "\n",
    "---\n",
    "**The emphasis is on developing intuition for**:\n",
    "- how a forward computation defines a loss,\n",
    "- how `autograd` constructs the computation graph dynamically,\n",
    "- how `loss.backward()` traverses this graph using the chain rule,\n",
    "- how gradients acccumulate on parameters,\n",
    "- and how parameter updates translate gradients into learning\n",
    "\n",
    "All of this is done **without** relying on high-level abstractions such as optimisers or nueral network modules.\n",
    "\n",
    "---\n",
    "**Key ideas explored include**:\n",
    "- defining a minimal model with explicit parameters,\n",
    "- computing a scalar loss and invoking backpropogation,\n",
    "- understanding leaf tensors and gradient accumulation,\n",
    "- manually updating parameters using `torch.no_grad`,\n",
    "- observing the effects of learning rate on convergence and stability,\n",
    "- and recognising training loops as repeated applications of simple gradient-based updates.\n",
    "---\n",
    "**Conceptual role in the series**:\n",
    "\n",
    "This tutorial serves as the **conceptual anchor** for the rest of the Part 1.\n",
    "\n",
    "It connects:\n",
    "- basic tensor operations (Tutorial 1),\n",
    "- to computation graphs and gradient flow (Tutorial 3),\n",
    "- and prepares the ground for understanding gradients beyond scalar losses (Tutorial 4).\n",
    "\n",
    "While optimisation appears here for the first time, it is introduced in its **simplest possible form**, so that later discussions of gradient structure, sensitivity, and objective design have a clear reference point.\n",
    "\n",
    "---\n",
    "**Scope and exclusions**\n",
    "\n",
    "The focus is intentionally narrow:\n",
    "- no neural network modules,\n",
    "- no datasets or data loaders,\n",
    "- no built-in optimisers.\n",
    "\n",
    "The purpose it to understand *what learning is doing*, not how to scale it.\n",
    "\n",
    "---\n",
    "**Recommended prerequisites**\n",
    "- Familiarity with PyTorch tensors and basic operations\n",
    "- Comfort with scalar functions and derivatives\n",
    "- Basic understanding of gradient descent as an idea\n",
    "\n",
    "---\n",
    "**Author**: Angze Li\n",
    "\n",
    "**Last updated**: 2026-02-20\n",
    "\n",
    "**Version**: v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26685db-2781-4046-88a8-b2c42d5ef99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad008c21-7229-439e-bd87-f464c7a4dd80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. What problem are we setting up?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876467a1-4463-4938-bcdb-1c701ea01c62",
   "metadata": {},
   "source": [
    "We consider a very small model with:\n",
    "- an input vector `x`\n",
    "- trainable parameters `w` and `b`\n",
    "- a loss function that measures how wrong the model output is\n",
    "\n",
    "The model produces an output by combining the input with its parameters:\n",
    "\n",
    "- `w` (the **weights**) determine how strongly each component of the input `x` influences the output.\n",
    "- `b` (the **bias**) provides an offset that allows the model to shift its output independently of the input.\n",
    "- Together, `w` and `b` define the behaviour of the model and are the quantities we want to learn.\n",
    "\n",
    "The **loss function** compares the model output to the expected output and returns a single scalar value that quantifies the error.  \n",
    "This scalar loss is what drives learning: by analysing how the loss changes with respect to `w` and `b`, we can adjust the parameters to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9737db3-2299-4668-8fe7-d1eae3163ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and target\n",
    "x = torch.ones(5)      # input features\n",
    "y = torch.zeros(3)    # expected output (target)\n",
    "\n",
    "# Trainable parameters\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584dbdf-cab7-4e65-9191-f63f87e22a18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What these tensors represent\n",
    "- `x` is a fixed input vector.\n",
    "- `w` is a weight matrix mapping 5 inputs to 3 outputs.\n",
    "- `b` is a bias vector added to the output. (Note the dimensionalities of `x`, `w` and `b`.)\n",
    "- `requires_grad=True` tells PyTorch:\n",
    "  \n",
    "  > ‚ÄúTrack how this tensor influences the final loss.‚Äù\n",
    "\n",
    "Only tensors with `requires_grad=True` will receive gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ca49d8-d7cc-4b6c-aeaa-66e508e02fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output z: tensor([ 0.8304,  4.3978, -0.1251], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(\"Model output z:\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f6133-2872-4044-b2bf-e2ac9ee8ac69",
   "metadata": {},
   "source": [
    "### What just happened mathematically\n",
    "\n",
    "This line computes the **forward pass** of the model:\n",
    "\n",
    "$$\n",
    "z = x W + b\n",
    "$$\n",
    "\n",
    "- `x` is shape `(5,)`\n",
    "- `w` is shape `(5, 3)`\n",
    "- result `z` is shape `(3,)`\n",
    "\n",
    "Each element of `z` depends on **all entries of `w` and `b`**.\n",
    "\n",
    "PyTorch remembers this dependency internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349a5ac-eb4e-48ff-b388-02c4db1d6b1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Loss: Why we need it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2a895f-737e-49a5-9f08-169b7dc6e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(2.0782, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5e58f-4f9b-4fa6-bd67-ca25e1e6f5ac",
   "metadata": {},
   "source": [
    "The loss is a **single scalar number** that measures how far the model output `z`\n",
    "is from the target `y`.\n",
    "\n",
    "In this example, we use **binary cross-entropy with logits** (hence the mame of the function `torch.nn.functional.binary_cross_entropy_with_logits()`) , which is computed *element-wise* as:\n",
    "\n",
    "$$\n",
    "\\ell(z_i, y_i) = - \\left[ y_i \\ln(\\sigma(z_i)) + (1 - y_i)\\ln(1 - \\sigma(z_i)) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function,\n",
    "- $z_i$ is the model output (logit) for the $i$-th entry,\n",
    "- $y_i \\in \\{0, 1\\}$ is the corresponding target.\n",
    "\n",
    "The total loss is obtained by averaging over all entries:\n",
    "\n",
    "$$\n",
    "\\text{loss} = \\frac{1}{N} \\sum_i \\ell(z_i, y_i)\n",
    "$$\n",
    "\n",
    "Key points:\n",
    "- The loss is a **scalar**, which is required for backpropagation.\n",
    "- Using logits (raw values `z`) instead of probabilities is **numerically more stable**.\n",
    "- PyTorch combines the sigmoid operation and cross-entropy computation internally.\n",
    "\n",
    "At this point, PyTorch has built a **computation graph** linking  \n",
    "`w, b ‚Üí z ‚Üí loss`.\n",
    "\n",
    "One can manually verify that the loss computed above follows this formula exactly by applying the sigmoid function to $z$ and evaluating the expression entry by entry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea953f-e90b-4a52-90fa-3cb04ee81ba4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8384c8-66aa-494e-a527-d94ca0144bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w before backward: None\n",
      "Gradient of b before backward: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient of w before backward:\", w.grad)\n",
    "print(\"Gradient of b before backward:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4be57-8e53-4ab7-8619-31e86c03a3c2",
   "metadata": {},
   "source": [
    "### Why gradients are `None` (for now)\n",
    "\n",
    "Gradients are not computed automatically.\n",
    "\n",
    "Up to now, we have only done a **forward pass**.\n",
    "No differentiation has happened yet.\n",
    "\n",
    "To compute gradients, we must explicitly ask PyTorch to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "600dca61-faef-4cad-ac6c-a7a7292b2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c362ead-8eec-4296-8a80-24763e5b1c57",
   "metadata": {},
   "source": [
    "### What `backward()` actually means\n",
    "\n",
    "Calling `loss.backward()` tells PyTorch to compute **how the loss changes with respect to each trainable parameter**.\n",
    "\n",
    "More concretely, PyTorch performs the following steps:\n",
    "\n",
    "1. **Start from the scalar `loss`**  \n",
    "   The loss is the final output of the computation graph. Gradients are always computed with respect to a scalar, because only then is the gradient well-defined.\n",
    "\n",
    "2. **Traverse the computation graph backwards**  \n",
    "   PyTorch follows the recorded operations in reverse order, moving from the loss back toward the parameters (`w` and `b`).\n",
    "\n",
    "3. **Apply the chain rule automatically**  \n",
    "   At each operation, PyTorch computes local derivatives and combines them using the chain rule, propagating gradients backward through the graph.\n",
    "\n",
    "4. **Accumulate gradients in leaf tensors**  \n",
    "   For every tensor marked with `requires_grad=True` that is a *leaf* of the graph (such as `w` and `b`), PyTorch stores the resulting gradients in the `.grad` attribute.\n",
    "\n",
    "After this process:\n",
    "- `w.grad` contains $\\partial\\ \\text{loss}/\\partial w$\n",
    "- `b.grad` contains $\\partial\\ \\text{loss}/\\partial b$\n",
    "\n",
    "This entire backward pass from the loss back to the parameters is known as **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b174b4-b096-42ca-b726-3a611a3657b5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Definition of leaf tensors</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Leaf tensors**:\n",
    "- are created directly by the user (or by detaching from a graph),\n",
    "- are **not** the result of any `autograd`-tracked operation,\n",
    "- are the tensors where gradients are **accumulated** during backpropagation.\n",
    "\n",
    "In our example so far, `w` and `b` are created directly using `torch.randn()` with `requires_grad=True`, and therefore they are **leaf tensors**.\n",
    "\n",
    "By contrast, `loss` is produced as the result of a sequence of operations (e.g., `binary_cross_entropy_with_logits`), so it is **not** a leaf tensor. Instead, it stores a `grad_fn` that defines how gradients should be propagated backward through the computation graph.\n",
    "\n",
    "You can check whether a tensor is a leaf tensor using:\n",
    "```python\n",
    "tensor.is_leaf\n",
    "```\n",
    "\n",
    "The result would be either `True` or `False`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a4c464-17e8-472c-b168-e7c12fba6280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w:\n",
      " tensor([[0.2321, 0.3293, 0.1563],\n",
      "        [0.2321, 0.3293, 0.1563],\n",
      "        [0.2321, 0.3293, 0.1563],\n",
      "        [0.2321, 0.3293, 0.1563],\n",
      "        [0.2321, 0.3293, 0.1563]])\n",
      "Gradient of b:\n",
      " tensor([0.2321, 0.3293, 0.1563])\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient of w:\\n\", w.grad)\n",
    "print(\"Gradient of b:\\n\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b8c1f-cac5-4ece-a59f-5ce0a564bf9d",
   "metadata": {},
   "source": [
    "### How to interpret these gradients\n",
    "- `w.grad` tells us how the loss changes if each entry of `w` is *increased* slightly.\n",
    "- `b.grad` tells us how the loss changes if each entry of `b` is *increased* slightly.\n",
    "\n",
    "The gradients have the **same shape** as the parameters they belong to.\n",
    "This is not an accident ‚Äî it allows **parameter-wise** updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177bf267-d5f3-46d5-80a6-5b926861c4b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Learning rate and `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c2b15-02fe-463d-a7be-973a38b3b0ab",
   "metadata": {},
   "source": [
    "The variable `learning_rate` controls **how large each parameter update is**.  \n",
    "It determines how far we move the parameters in the direction that reduces the loss.\n",
    "\n",
    "- A larger learning rate leads to **bigger steps**, which may speed up learning but risk overshooting.\n",
    "- A smaller learning rate leads to **smaller, more cautious steps**, which are more stable but slower.\n",
    "\n",
    "In this example, `learning_rate = 0.1` is chosen as a simple, reasonable value to clearly illustrate the update step.\n",
    "\n",
    "---\n",
    "By default, all tensors with `requires_grad=True` are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, because, e.g.:\n",
    "- Updating parameters is **not** part of the model itself, i.e., we only want to to forward computations through the network.\n",
    "- We do not want PyTorch to track these operations\n",
    "\n",
    "Otherwise, the computation graph would grow incorrectly.\n",
    "\n",
    "Expand the two cells below to learn more about learning rate and the necessity of `torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc990303-fdc1-4b49-ac32-d2d52b50f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "    b -= learning_rate * b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2b982-acc3-4dde-bb76-77abc4a0e20c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Learning rate: scale, intuition, and practical behaviour</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### What is the usual range of the learning rate?\n",
    "\n",
    "There is no single ‚Äúcorrect‚Äù learning rate, but in practice:\n",
    "\n",
    "- Common values range from **1e‚àí4 to 1e‚àí1**\n",
    "- Typical starting points are **0.1**, **0.01**, or **0.001**\n",
    "- The appropriate value depends on:\n",
    "  - the model,\n",
    "  - the loss function,\n",
    "  - and the scale of the gradients\n",
    "\n",
    "In this notebook, we use `learning_rate = 0.1` purely for illustration, not because it is optimal.\n",
    "\n",
    "---\n",
    "\n",
    "#### Intuition: the hill analogy\n",
    "\n",
    "Imagine the loss as a landscape with hills and valleys:\n",
    "\n",
    "- The **height** represents the loss value\n",
    "- The **slope** represents the gradient\n",
    "- The parameters (`w`, `b`) represent your current position on the hill\n",
    "\n",
    "The learning rate controls **how large a step you take downhill**:\n",
    "\n",
    "- Too small ‚Üí you move very slowly and make little progress\n",
    "- Too large ‚Üí you may overshoot the valley and oscillate\n",
    "- Just right ‚Üí you move steadily toward a minimum\n",
    "\n",
    "---\n",
    "\n",
    "#### Why the learning rate matters\n",
    "\n",
    "The gradient tells you *which direction* reduces the loss.  \n",
    "The learning rate determines *how far* you move in that direction.\n",
    "\n",
    "Effective learning requires **both** a meaningful gradient and an appropriate learning rate.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014de5ed-f1f0-4a41-bbac-bcef3128db5d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Why <code>torch.no_grad()</code> is needed during parameter updates</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "We use `torch.no_grad()` because updating model parameters is **not part of the forward computation** that defines the model itself.\n",
    "\n",
    "During the forward pass, PyTorch records operations to build a computation graph.  \n",
    "However, parameter updates are a **bookkeeping step** performed after gradients have been computed.\n",
    "\n",
    "If parameter updates were performed without `torch.no_grad()`:\n",
    "- PyTorch would attempt to track these updates as part of the computation graph\n",
    "- The graph would incorrectly include the update operations themselves\n",
    "- Subsequent calls to `backward()` could fail or produce incorrect gradients\n",
    "\n",
    "Using `torch.no_grad()` temporarily disables gradient tracking, ensuring that:\n",
    "- parameter updates do not become part of the computation graph\n",
    "- each training step starts from a clean graph\n",
    "- gradients are computed only for the forward computation\n",
    "\n",
    "This separation between **model computation** and **parameter updates** is essential for correct and stable training.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eaab674-fbec-4f82-b462-4bbd7b56127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " tensor([[ 1.1313,  0.2263,  0.4681],\n",
      "        [ 0.0746,  1.6689, -1.3671],\n",
      "        [ 2.1811, -0.7918,  0.8280],\n",
      "        [-0.3393,  1.7619, -0.0465],\n",
      "        [-1.0870, -0.3548, -0.1821]], requires_grad=True)\n",
      "b:\n",
      " tensor([-1.2696,  1.6897,  0.0806], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "print(\"w:\\n\", w)\n",
    "print(\"b:\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3636e4-b723-49ba-b472-1954cc187208",
   "metadata": {},
   "source": [
    "### Why gradients must be cleared\n",
    "\n",
    "By default, PyTorch **accumulates gradients**.\n",
    "\n",
    "This means:\n",
    "- Calling `backward()` multiple times without resetting gradients will **add new gradients** to the existing values stored in `.grad`\n",
    "- This behaviour is intentional and allows gradients to be accumulated across multiple backward passes (e.g. when summing losses)\n",
    "\n",
    "However, for most training loops‚Äîespecially when learning‚Äîthis accumulation can be confusing and lead to subtle bugs.\n",
    "\n",
    "Clearing gradients explicitly ensures that each backward pass computes gradients **only for the current forward computation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a2516-51fd-4bb8-865d-63e34dc6c9f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Make it a loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee69a4-32bd-47d4-8a46-00923e3138fa",
   "metadata": {},
   "source": [
    "So far, we have seen how to compute gradients and perform a **single parameter update**.\n",
    "\n",
    "Learning, however, does not happen in one step.  \n",
    "Instead, the process of:\n",
    "1. computing the loss,\n",
    "2. computing gradients via backpropagation,\n",
    "3. updating parameters,\n",
    "\n",
    "is repeated **many times**.\n",
    "\n",
    "Each repetition is called a **training step** (or iteration).  \n",
    "Over successive steps, the parameters are gradually adjusted to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e1c3d0-100c-463b-86a3-e0ddb3e59553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0 | Loss: 1.967333\n",
      "Step  1 | Loss: 1.860896\n",
      "Step  2 | Loss: 1.758716\n",
      "Step  3 | Loss: 1.660594\n",
      "Step  4 | Loss: 1.566339\n",
      "Step  5 | Loss: 1.475782\n",
      "Step  6 | Loss: 1.388779\n",
      "Step  7 | Loss: 1.305222\n",
      "Step  8 | Loss: 1.225037\n",
      "Step  9 | Loss: 1.148188\n",
      "Step 10 | Loss: 1.074674\n",
      "Step 11 | Loss: 1.004523\n",
      "Step 12 | Loss: 0.937784\n",
      "Step 13 | Loss: 0.874519\n",
      "Step 14 | Loss: 0.814791\n",
      "Step 15 | Loss: 0.758652\n",
      "Step 16 | Loss: 0.706131\n",
      "Step 17 | Loss: 0.657226\n",
      "Step 18 | Loss: 0.611897\n",
      "Step 19 | Loss: 0.570063\n"
     ]
    }
   ],
   "source": [
    "# Simple training loop to demonstrate learning\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_steps = 20\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Forward pass\n",
    "    z = torch.matmul(x, w) + b\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameter update\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Clear gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # Monitor progress\n",
    "    print(f\"Step {step:2d} | Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60593910-18e6-49ad-99e3-89e1ed30d115",
   "metadata": {},
   "source": [
    "### The training loop: how learning emerges\n",
    "- Each loop iteration performs one **forward‚Äìbackward‚Äìupdate** cycle.\n",
    "- The loss is recomputed using the updated parameters at every step.\n",
    "- If the learning rate is reasonable, the loss should gradually decrease.\n",
    "\n",
    "This loop demonstrates the core mechanism behind training neural networks:\n",
    "repeated application of backpropagation and parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b435b-1a03-45c0-9828-70e0f5a7d721",
   "metadata": {},
   "source": [
    "### What backpropagation really did here\n",
    "\n",
    "- We computed a forward pass to get a scalar loss\n",
    "- PyTorch recorded how that loss depends on parameters\n",
    "- `loss.backward()` computed all required gradients automatically\n",
    "- Gradients told us how to update parameters to reduce the loss\n",
    "\n",
    "No neural network magic ‚Äî just calculus applied systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260defd0-928d-4eff-9bac-c7c028642906",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Convergence in gradient-based learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52234924-84bf-4bae-978a-6faa2018ee93",
   "metadata": {},
   "source": [
    "At each iteration:\n",
    "1. A forward pass computes the model output and the loss.\n",
    "2. Backpropagation computes gradients of the loss with respect to the parameters.\n",
    "3. The parameters are updated in the direction that reduces the loss.\n",
    "4. Gradients are cleared in preparation for the next step.\n",
    "\n",
    "As the loop progresses, the loss stored in `loss_history` decreases and eventually falls below the chosen threshold.  \n",
    "This indicates that successive parameter updates are making **smaller and smaller improvements** (see the figure below for the visualisation of decrease in `loss`), and the model is approaching a minimum of the loss function.\n",
    "\n",
    "The stopping condition is:\n",
    "```python\n",
    "loss_value > 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef2b3054-0ed7-412f-bcc1-ab9dcbf567f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0 | Loss: 0.509305\n",
      "Step  100 | Loss: 0.042671\n",
      "Step  200 | Loss: 0.022983\n",
      "Step  300 | Loss: 0.015772\n",
      "Step  400 | Loss: 0.012007\n",
      "\n",
      "Total learning steps: 486\n",
      "Final loss: 0.009982\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 49.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3) \n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "loss_value = float(\"inf\")\n",
    "learning_step = 0\n",
    "max_steps = 1000\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "while loss_value > 0.01 and learning_step < max_steps:\n",
    "    # Forward pass\n",
    "    z = torch.matmul(x, w) + b\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameter update\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Clear gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # Extract scalar loss\n",
    "    loss_value = loss.item()\n",
    "    loss_history.append(loss_value)\n",
    "\n",
    "    # Monitor progress\n",
    "    if learning_step % 100 == 0:\n",
    "        print(f\"Step {learning_step:4d} | Loss: {loss_value:.6f}\")\n",
    "\n",
    "    learning_step += 1\n",
    "\n",
    "print(\"\\nTotal learning steps:\", learning_step)\n",
    "print(\"Final loss:\", round(loss_value, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60be9273-16b0-4fa6-866d-e1bb3bd1fd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "ax = plt.gca()\n",
    "for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1.8)\n",
    "\n",
    "plt.plot(np.linspace(0, learning_step, learning_step), loss_history, linewidth=2.5, label=\"Loss over training steps\")\n",
    "plt.xlabel(\"Training step\", fontsize=22, fontweight='bold')\n",
    "plt.ylabel(\"Loss\", fontsize=22, fontweight='bold')\n",
    "plt.xticks(fontsize=14, fontweight='bold')\n",
    "plt.yticks(fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"upper right\", prop={\"size\": 12, \"weight\": \"bold\"})\n",
    "plt.title(\"Convergence of loss over training\", fontsize=22, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5ee24-6238-43e5-a5f6-e765cf113666",
   "metadata": {},
   "source": [
    "### The effect of learning rate\n",
    "The experiment below examines how the **learning rate** affects convergence in gradient-based learning.\n",
    "\n",
    "For each value in `learning_rates`, the model is re-initialised and trained under identical conditions. This ensures that any differences in behaviour arise solely from the choice of learning rate, rather than from initialisation or model structure.\n",
    "\n",
    "The reported results show how many steps are required for the loss to fall below the threshold\n",
    "```python\n",
    "loss_value > 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "776c10ce-10d8-4aaa-8433-a624d48b82ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate = 0.001 | steps = 1000 | final loss = 0.278654\n",
      "learning_rate = 0.01  | steps = 1000 | final loss = 0.049824\n",
      "learning_rate = 0.1   | steps = 526  | final loss = 0.009993\n",
      "learning_rate = 1.0   | steps = 45   | final loss = 0.009884\n",
      "learning_rate = 10    | steps = 2    | final loss = 0.000004\n",
      "learning_rate = 100   | steps = 2    | final loss = 0.000000\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "max_steps = 1000\n",
    "threshold = 0.01\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Re-initialise model for fair comparison\n",
    "    x = torch.ones(5)\n",
    "    y = torch.zeros(3) \n",
    "    w = torch.randn(5, 3, requires_grad=True)\n",
    "    b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "    loss_value = float(\"inf\")\n",
    "    learning_step = 0\n",
    "    loss_history = []\n",
    "\n",
    "    while loss_value > threshold and learning_step < max_steps:\n",
    "        # Forward pass\n",
    "        z = torch.matmul(x, w) + b\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameter update\n",
    "        with torch.no_grad():\n",
    "            w -= lr * w.grad\n",
    "            b -= lr * b.grad\n",
    "\n",
    "        # Clear gradients\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "        # Extract scalar loss\n",
    "        loss_value = loss.item()\n",
    "        loss_history.append(loss_value)\n",
    "\n",
    "        learning_step += 1\n",
    "\n",
    "    print(f\"learning_rate = {lr:<5} | steps = {learning_step:<4} | final loss = {loss_value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862089a-094c-4a2a-8e06-3c42d28a4e1b",
   "metadata": {},
   "source": [
    "### Interpreting learning rate effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ea0b0-0eb0-48a3-a56a-b01e95a41fbd",
   "metadata": {},
   "source": [
    "In this simple and well-conditioned example, larger learning rates reduce the number of steps required for convergence, allowing the loss to fall below the chosen threshold more quickly.\n",
    "\n",
    "However, this behaviour does not generalise to all optimisation problems.  In more complex loss landscapes, overly large learning rates can lead to:\n",
    "- unstable updates,\n",
    "- divergence of the loss,\n",
    "- or convergence to poor solutions that generalise poorly.\n",
    "\n",
    "For this reason, smaller learning rates are often preferred in practice, as they provide greater **robustness and stability** across a wide range of models and training conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33f4f1-d9a7-4315-82cd-9b477a7e2e18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7. Extra: Inspecting the computation graph with `grad_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8344547-26a9-467b-a15d-d14c690d009d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.grad_fn: <AddBackward0 object at 0x00000249765D4640>\n",
      "w.grad_fn: None\n",
      "b.grad_fn: None\n",
      "z.grad_fn after detach(): None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5)\n",
    "y = torch.zeros(3) \n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "z = torch.matmul(x, w) + b\n",
    "print(\"z.grad_fn:\", z.grad_fn)\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "    b -= learning_rate * b.grad\n",
    "\n",
    "print(\"w.grad_fn:\", w.grad_fn)\n",
    "print(\"b.grad_fn:\", b.grad_fn)\n",
    "\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "loss_value = loss.item()\n",
    "\n",
    "z = z.detach()\n",
    "print(\"z.grad_fn after detach():\", z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68bae0-798e-4682-bae6-7c2b330bea90",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>How computation graphs track and control gradient flow</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the code above, we observe the following:\n",
    "\n",
    "```text\n",
    "z.grad_fn: <AddBackward0 object at 0x...>\n",
    "w.grad_fn: None\n",
    "b.grad_fn: None\n",
    "```\n",
    "#### Why does `z.grad_fn` exist, but `w.grad_fn` and `b.grad_fn` do not?\n",
    "The tensor `z` is computed as the result of operations:\n",
    "\n",
    "```python\n",
    "z = torch.matmul(x, w) + b\n",
    "```\n",
    "\n",
    "Because `z` is produced by operations, PyTorch attaches a **gradient function** to it.\n",
    "This function, accessible via `z.grad_fn`, records how `z` was computed and how gradients should be propagated backward during backpropagation.\n",
    "\n",
    "By contrast, `w` and `b` are **leaf tensors** and therefore have `grad_fn = None`, even though they still receive gradients via their `.grad` attributes after calling `backward()`.\n",
    "\n",
    "#### Breaking the computation graph with `detach()`\n",
    "The method `detach()` creates a new tensor that shares the same underlying data but is disconnected from the computation graph.\n",
    "\n",
    "In this example, we explicitly overwrite `z`:\n",
    "```python\n",
    "z = z.detach()\n",
    "```\n",
    "After detaching:\n",
    "- `z` is treated as a constant,\n",
    "- no gradients will flow backward through it,\n",
    "- and operations involving `z` will not be tracked by autograd.\n",
    "This is useful when you intentionally want to stop gradient flow or avoid growing the computation graph.\n",
    "\n",
    "#### What does <AddBackward0 object at 0x...> mean?\n",
    "The object `<AddBackward0>` represents the backward rule for the addition operation that produced `z`.\n",
    "\n",
    "In this example, the final operation was:\n",
    "```Python\n",
    "z = (torch.matmul(x, w)) + b\n",
    "```\n",
    "PyTorch therefore attaches an **AddBackward function** to `z`, which knows how to:\n",
    "- apply the chain rule,\n",
    "- and propagate gradients from `z` to its inputs during backpropagation.\n",
    "\n",
    "The hexadecimal value (e.g., 0x32d6da620) is simply the memory address of this Python object during the current execution.\n",
    "\n",
    "It has no mathematical meaning and can be safely ignored.\n",
    "\n",
    "There are many other functions besides `<AddBackward0>`.\n",
    "\n",
    "Each differentiable operation (e.g. matrix multiplication, activation functions, loss functions) has its own corresponding backward function, which appears as the `grad_fn` of the resulting tensor. For example, the tensor loss has a different `grad_fn` reflecting the loss operation used to compute it.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4becea22-e217-46d6-b30f-315e29a019f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.grad_fn: <BinaryCrossEntropyWithLogitsBackward0 object at 0x00000249765678E0>\n"
     ]
    }
   ],
   "source": [
    "print(\"loss.grad_fn:\", loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ea2f0-60ef-41c3-8124-7e296a740776",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üß≠ Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ae866-80bf-49e8-8924-d56362baae95",
   "metadata": {},
   "source": [
    "In this tutorial, you have seen learning in its simplest possible form: a small set of parameters, a scalar loss, and gradients computed via backpropogation.\n",
    "\n",
    "Nothing here was hidden behind abstractions. Every step‚Äîfrom the forward computation, to gradient calculation, to parameter updates‚Äîwas written explicitly so that the mechanics of learning could be observed directly.\n",
    "\n",
    "This minimal setting is intentional.\n",
    "\n",
    "Once the basic loop\n",
    "> compute ‚Üí differentiate ‚Üí update\n",
    "\n",
    "is understood at this level, more complex models add structure, not fundamentally new ideas\n",
    "\n",
    "In the next tutorials, we will move beyond this scalar setting. We will open the computation graph, inspect how `autograd` represents and traverses it, and study gradients not just as update rules, but as structured objects that carry geometric and statistical meanings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
